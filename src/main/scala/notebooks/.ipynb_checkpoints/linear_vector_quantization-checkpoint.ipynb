{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Vector Quantization\n",
    "\n",
    "One of the biggest disadvantages of _k_-Nearest Neighbors is that the training data must be kept in its entirety for inference, given that it isn't used at all to train the algorithm. Thus, as the volume of data grows, also does the prediction time. \n",
    "\n",
    "In order to combat this issue, we have a very similar algorithm called __Linear Vector Quantization__. In a nutshell, what LVQ does is keeping a subset of the data that best represents the patterns and nuances of it. It, then, at inference time, uses the same methodology of _k_-Nearest Neighbors to produce either a category or a value.\n",
    "\n",
    "As happens in KNN, LVQ makes predictions by finding the best match among the library or collection of patterns. The difference is that this collection of patterns is learned from the training data, and it is called _codebook vectors_. Hence, each patterns in this collection is called a _codebook_.\n",
    "\n",
    "These _codebook vectors_ are initialized as randomly selected examples from the training set, and then are tuned during a given number of epochs.\n",
    "\n",
    "Once the _codebook vectors_ have been prepared, then the _k_-Nearest Neighbors inference algorithm is used with _k=1_. Although LVQ was initially developed for classification tasks, it can also be used for regression problems. \n",
    "\n",
    "Let's start our implementation by loading the code and libraries we'll need. We will build our solution on top of the ones we implemented in the [previous notebook](https://github.com/jesus-a-martinez-v/toy-ml/blob/master/src/main/scala/notebooks/k_nearest_neighbors.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                                            , KNearestNeighbors._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.github.tototoshi::scala-csv:1.3.5`\n",
    "import $file.^.datasmarts.ml.toy.scripts.KNearestNeighbors, KNearestNeighbors._\n",
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We'll use the [Ionosphere](https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data) dataset. It involves the prediction of structure in the atmosphere given radar returns targeting free electrons in the ionosphere. It is a binary classification task.\n",
    "\n",
    "Let's load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in dataset: 351\n",
      "Number of columns in dataset: 35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mBASE_DATA_PATH\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"../../resources/data\"\u001b[39m\n",
       "\u001b[36mionospherePath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"../../resources/data/14/ionosphere.csv\"\u001b[39m\n",
       "\u001b[36mrawData\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\n",
       "    Text(1),\n",
       "    Text(0),\n",
       "    Text(0.99539),\n",
       "    Text(-0.05889),\n",
       "    Text(0.85243),\n",
       "    Text(0.02306),\n",
       "    Text(0.83398),\n",
       "    Text(-0.37708),\n",
       "    Text(1),\n",
       "    Text(0.03760),\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mnumberOfRows\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m351\u001b[39m\n",
       "\u001b[36mnumberOfColumns\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m35\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\n",
       "    Numeric(1.0),\n",
       "    Numeric(0.0),\n",
       "    Numeric(0.99539),\n",
       "    Numeric(-0.05889),\n",
       "    Numeric(0.85243),\n",
       "    Numeric(0.02306),\n",
       "    Numeric(0.83398),\n",
       "    Numeric(-0.37708),\n",
       "    Numeric(1.0),\n",
       "    Numeric(0.0376),\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mlookUpTable\u001b[39m: \u001b[32mMap\u001b[39m[\u001b[32mData\u001b[39m, \u001b[32mInt\u001b[39m] = \u001b[33mMap\u001b[39m(Text(g) -> \u001b[32m0\u001b[39m, Text(b) -> \u001b[32m1\u001b[39m)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val BASE_DATA_PATH = \"../../resources/data\"\n",
    "val ionospherePath = s\"$BASE_DATA_PATH/14/ionosphere.csv\"\n",
    "\n",
    "val rawData = loadCsv(ionospherePath)\n",
    "val numberOfRows = rawData.length\n",
    "val numberOfColumns = rawData.head.length\n",
    "println(s\"Number of rows in dataset: $numberOfRows\")\n",
    "println(s\"Number of columns in dataset: $numberOfColumns\")\n",
    "\n",
    "val (data, lookUpTable) = {\n",
    "    val dataWithNumericColumns = (0 until (numberOfColumns - 1)).toVector.foldLeft(rawData) { (d, i) => textColumnToNumeric(d, i) }\n",
    "    categoricalColumnToNumeric(dataWithNumericColumns, numberOfColumns - 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance\n",
    "\n",
    "In this notebook we'll use Euclidean distance as a similarity measure between two rows or vectors. Here's the equation:\n",
    "\n",
    "$$ distance(X,Y) = \\sqrt{\\sum_{i=1}^n{(X_i - Y_i)^2}}$$\n",
    "\n",
    "We implemented this function in the [previous notebook](https://github.com/jesus-a-martinez-v/toy-ml/blob/master/src/main/scala/notebooks/k_nearest_neighbors.ipynb). Please take a look at it before moving on ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Matching Unit\n",
    "\n",
    "The Best Matching Unit is the codebook vector that is most similar to a new piece of data. In order to determine this BMU we must calculate the measure of similarity between the example or new piece of data and each codebook vector.\n",
    "\n",
    "Le's implement a function to get the Best Matching Unit for a particular example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mgetBestMatchingUnit\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getBestMatchingUnit(codebooks: Vector[Vector[Numeric]], testRow: Vector[Numeric]) = {\n",
    "  val codebooksDistances = for {\n",
    "    codebook <- codebooks\n",
    "  } yield {\n",
    "    val distance = euclideanDistance(codebook, testRow)\n",
    "    (codebook, distance)\n",
    "  }\n",
    "\n",
    "  codebooksDistances.minBy(_._2)._1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Let's test it with a mock dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.3290173915275787\n",
      "1.9494646655653247\n",
      "1.5591439385540549\n",
      "0.5356280721938492\n",
      "4.850940186986411\n",
      "2.592833759950511\n",
      "4.214227042632867\n",
      "6.522409988228337\n",
      "4.985585382449795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mmockDataset\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mNumeric\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m2.7810836\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m2.550537003\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m1.465489372\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m2.362125076\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m3.396561688\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m4.400293529\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m1.38807019\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.850220317\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m3.06407232\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m3.005305973\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m7.627531214\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m2.759262235\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m5.332441248\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m2.088626775\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m6.922596716\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.77106367\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m8.675418651\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m-0.242068655\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m7.673756466\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m3.508563011\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m))\n",
       ")\n",
       "\u001b[36mtestRow\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mNumeric\u001b[39m] = \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m2.7810836\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m2.550537003\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mockDataset = Vector(\n",
    "  (2.7810836, 2.550537003, 0),\n",
    "  (1.465489372, 2.362125076, 0),\n",
    "  (3.396561688, 4.400293529, 0),\n",
    "  (1.38807019, 1.850220317, 0),\n",
    "  (3.06407232, 3.005305973, 0),\n",
    "  (7.627531214, 2.759262235, 1),\n",
    "  (5.332441248, 2.088626775, 1),\n",
    "  (6.922596716, 1.77106367, 1),\n",
    "  (8.675418651, -0.242068655, 1),\n",
    "  (7.673756466, 3.508563011, 1)\n",
    ") map { case (x1, x2, y) => Vector(Numeric(x1), Numeric(x2), Numeric(y))}\n",
    "\n",
    "val testRow = mockDataset.head\n",
    "\n",
    "mockDataset.foreach { r => \n",
    "    println(euclideanDistance(testRow, r))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres4\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mNumeric\u001b[39m] = \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m2.7810836\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m2.550537003\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getBestMatchingUnit(mockDataset, mockDataset.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Codebooks Vectors\n",
    "\n",
    "The first step is to initialize a set of codebook vectors with random features extracted from the training set. Let's now implement a function that does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mrandomCodebook\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def randomCodebook(train: Dataset) = {\n",
    "  val numberOfRecords = train.length\n",
    "  val numberOfFeatures = train.head.length\n",
    "\n",
    "  (0 until numberOfFeatures).map { index =>\n",
    "    train(Random.nextInt(numberOfRecords))(index)\n",
    "  }.toVector\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to adapt our randomly generated codebooks to best summarize or represent the training data. In order to do this, we'll apply the following iterative recipe:\n",
    "  \n",
    "  1. The BMU for each training example is found and __only this BMU is updated__.\n",
    "  2. The difference between the training example and the BMU is calculated. This is the __error__.\n",
    "  3. Their class values are compared. If it's a match, the error is added to the BMU to bring it closer to the training example. Otherwise, it is subtracted to push it farther away from the training pattern.\n",
    "  4. The __learning rate__ is used to control the porportion of adjustment to be applied to each BMU. So, for instance, a learning rate of 0.3 means that each BMU will be adjusted only by the 30% of the error between the BMUs and the training examples. \n",
    "  5. A decaying learning rate is used to prevent overshooting in the training process as we progress towards convergence. The formula used is:\n",
    "  \n",
    "      $$ rate = learningRate * (1 - \\frac{currentEpoch}{totalEpochs})$$\n",
    "      \n",
    "Let's create a function that performs the process described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtrainCodebooks\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trainCodebooks(train: Dataset, numberOfCodebooks: Int, learningRate: Double, numberOfEpochs: Int) = {\n",
    "  var codebooks = (0 until numberOfCodebooks).map(_ => randomCodebook(train).asInstanceOf[Vector[Numeric]]).toVector\n",
    "\n",
    "  for (epoch <- 0 until numberOfEpochs) {\n",
    "    val rate = learningRate * (1.0 - (epoch / numberOfEpochs))\n",
    "\n",
    "    for (row <- train) {\n",
    "      val numericRow = row.asInstanceOf[Vector[Numeric]]\n",
    "      var bestMatchingUnit = getBestMatchingUnit(codebooks, numericRow)\n",
    "      val bestMatchingUnitIndex = codebooks.indexOf(bestMatchingUnit)\n",
    "\n",
    "      val rowFeaturesIndices = row.indices.take(row.length - 2)\n",
    "      rowFeaturesIndices.foreach { i =>\n",
    "        val error = numericRow(i).value - bestMatchingUnit(i).value\n",
    "        val updatedValue = Numeric {\n",
    "          if (bestMatchingUnit.last == numericRow.last) {\n",
    "            bestMatchingUnit(i).value + error * rate\n",
    "          } else {\n",
    "            bestMatchingUnit(i).value - error * rate\n",
    "          }\n",
    "        }\n",
    "\n",
    "        bestMatchingUnit = updatedVector(bestMatchingUnit, updatedValue, i)\n",
    "      }\n",
    "\n",
    "      codebooks = updatedVector(codebooks, bestMatchingUnit, bestMatchingUnitIndex)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  codebooks\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this on our mock dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres7\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mNumeric\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m37.76095858243376\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m2.362125076\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m-25.08963663414342\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m3.005305973\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainCodebooks(mockDataset, 2, 0.3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "Let's implement a function that allow us to make predictions using our trained codebooks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mpredictWithCodebooks\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predictWithCodebooks(codebooks: Vector[Vector[Numeric]], testRow: Vector[Numeric]) = {\n",
    "  getBestMatchingUnit(codebooks, testRow).last\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mlearningVectorQuantization\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def learningVectorQuantization(train: Dataset, test: Dataset, parameters: Map[String, Any]) = {\n",
    "    val numberOfEpochs = parameters(\"numberOfEpochs\").asInstanceOf[Int]\n",
    "    val numberOfCodebooks = parameters(\"numberOfCodebooks\").asInstanceOf[Int]\n",
    "    val learningRate = parameters(\"learningRate\").asInstanceOf[Double]\n",
    "    \n",
    "  val codebooks = trainCodebooks(train, numberOfCodebooks, learningRate, numberOfEpochs)\n",
    "\n",
    "  test.map { row =>\n",
    "    predictWithCodebooks(codebooks, row.asInstanceOf[Vector[Numeric]])\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good.\n",
    "\n",
    "Let's now use our new algorithm to test it on the Ionosphere dataset.\n",
    "\n",
    "We'll start by running a baseline model on it and then our freshly implemented Linear Vector Quantization algorithm and then we will compare their performance.\n",
    "\n",
    "As a baseline for classification we will use a __zero rule classifier__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Rule Algorithm accuracy: 0.5633802816901409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mbaselineAccuracy\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.5633802816901409\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val baselineAccuracy = evaluateAlgorithmUsingTrainTestSplit[Numeric](\n",
    "        data, \n",
    "        (train, test, parameters) => zeroRuleClassifier(train, test), \n",
    "        Map.empty, \n",
    "        accuracy, \n",
    "        trainProportion=0.8)\n",
    "\n",
    "println(s\"Zero Rule Algorithm accuracy: $baselineAccuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Vector Quantization accuracy: 0.8732394366197183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mlinearVectorQuantizationAccuracy\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.8732394366197183\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val linearVectorQuantizationAccuracy = evaluateAlgorithmUsingTrainTestSplit[Numeric](\n",
    "    data,\n",
    "    learningVectorQuantization,\n",
    "    Map(\"numberOfEpochs\" -> 50, \"numberOfCodebooks\" -> 20, \"learningRate\" -> 0.3),\n",
    "    accuracy,\n",
    "    trainProportion=0.8)\n",
    "\n",
    "println(s\"Linear Vector Quantization accuracy: $linearVectorQuantizationAccuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there's a noticeable difference in performance between LVQ and our baseline Zero Rule Classifier.\n",
    "\n",
    "The magic of LVQ resides in its balance of simplicity and speed. For inference it uses a very simple and understandable approach, as happens with k-Nearest Neighbors, but it is much more efficient due to not being as _lazy_ and making some work beforehand determining the codebooks that best describe the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
