{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic Regression is, possibly, the simplest algorithm used for binary classification.\n",
    "\n",
    "Yeah, I agree. Although it has _regression_ on its name it is really a classification machine learning algorithm (weird...). At its core it uses the __logistic function__. As happened with __linear regression__, each logistic regression uses an equation as its representation. Each input variable is combined using coefficients to predict an output. Although the output is a real number, it is thresholded to turn it into either __0__ or __1__. Usually, the threshold used is 0.5.\n",
    "\n",
    "$$ \\hat{y} = \\frac{1}{1 + \\mathrm{e}^{-(b_0+b_1*x_1+...+b_n*x_n)}}$$\n",
    "\n",
    "In this equation $ \\mathrm{e} $ is the Euler's constant and the base of the natural logarithms. $ \\hat{y} $ is the prediction, $ b_0 $ is the bias term or intercept and $ b_1, ..., b_n$ are the coefficients for the variables $ x_1, ..., x_n$, correspondingly. \n",
    "\n",
    "How do we find the values of these coefficients? Using __gradient descent__, of course! For a deeper explanation, please refer to this [notebook](https://github.com/jesus-a-martinez-v/toy-ml/blob/master/src/main/scala/notebooks/multivariate_linear_regression.ipynb).\n",
    "\n",
    "The function to update each coefficient with gradient descent is:\n",
    "\n",
    "$$ b_i = b_i + \\alpha*(y - \\hat{y})*\\hat{y}*(1-\\hat{y})*x_i $$\n",
    "\n",
    "Let's start our implementation by loading the code and libraries we'll need. We will build our solution on top of the ones we implemented in the [previous notebook](https://github.com/jesus-a-martinez-v/toy-ml/blob/master/src/main/scala/notebooks/multivariate_linear_regression.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                                                       , MultivariateLinearRegression._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.github.tototoshi::scala-csv:1.3.5`\n",
    "import $file.^.datasmarts.ml.toy.scripts.MultivariateLinearRegression, MultivariateLinearRegression._\n",
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We'll use the [Pima Indians Diabetes](https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes) dataset. Let's load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in dataset: 768\n",
      "Number of column in dataset: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mBASE_DATA_PATH\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"../../resources/data\"\u001b[39m\n",
       "\u001b[36mpimaIndiansPath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"../../resources/data/9/pima-indians-diabetes.csv\"\u001b[39m\n",
       "\u001b[36mrawData\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\n",
       "    Text(6),\n",
       "    Text(148),\n",
       "    Text(72),\n",
       "    Text(35),\n",
       "    Text(0),\n",
       "    Text(33.6),\n",
       "    Text(0.627),\n",
       "    Text(50),\n",
       "    Text(1)\n",
       "  ),\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mnumberOfRows\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m768\u001b[39m\n",
       "\u001b[36mnumberOfColumns\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m9\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\n",
       "    Numeric(6.0),\n",
       "    Numeric(148.0),\n",
       "    Numeric(72.0),\n",
       "    Numeric(35.0),\n",
       "    Numeric(0.0),\n",
       "    Numeric(33.6),\n",
       "    Numeric(0.627),\n",
       "    Numeric(50.0),\n",
       "    Numeric(1.0)\n",
       "  ),\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val BASE_DATA_PATH = \"../../resources/data\"\n",
    "val pimaIndiansPath = s\"$BASE_DATA_PATH/9/pima-indians-diabetes.csv\"\n",
    "\n",
    "val rawData = loadCsv(pimaIndiansPath)\n",
    "val numberOfRows = rawData.length\n",
    "val numberOfColumns = rawData.head.length\n",
    "println(s\"Number of rows in dataset: $numberOfRows\")\n",
    "println(s\"Number of column in dataset: $numberOfColumns\")\n",
    "\n",
    "val data = (0 until numberOfColumns).toVector.foldLeft(rawData) { (d, i) => textColumnToNumeric(d, i)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Let's proceed to implement a function that makes prediction on a row, given some fitted coefficients. This will be very useful during the training phase as well as in the test stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mpredictLogisticRegression\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predictLogisticRegression(row: Vector[Data], coefficients: Vector[Double]): Double = {\n",
    "  val indices = row.indices.init\n",
    "\n",
    "  val yHat = indices.foldLeft(0.0) { (accumulator, index) =>\n",
    "    accumulator + coefficients(index + 1) * getNumericValue(row(index)).get\n",
    "  } + coefficients.head\n",
    "\n",
    "  1.0 / (1.0 + math.exp(-yHat))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on a mock dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=0.0, Predicted=0.2987569855650975 [0]\n",
      "Expected=0.0, Predicted=0.14595105593031163 [0]\n",
      "Expected=0.0, Predicted=0.08533326519733725 [0]\n",
      "Expected=0.0, Predicted=0.21973731424800344 [0]\n",
      "Expected=0.0, Predicted=0.24705900008926596 [0]\n",
      "Expected=1.0, Predicted=0.9547021347460022 [1]\n",
      "Expected=1.0, Predicted=0.8620341905282771 [1]\n",
      "Expected=1.0, Predicted=0.9717729050420985 [1]\n",
      "Expected=1.0, Predicted=0.9992954520878627 [1]\n",
      "Expected=1.0, Predicted=0.9054893228110497 [1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mmockDataset\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mNumeric\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m2.7810836\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m2.550537003\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m1.465489372\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m2.362125076\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m3.396561688\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m4.400293529\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m1.38807019\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.850220317\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m3.06407232\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m3.005305973\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m7.627531214\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m2.759262235\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m5.332441248\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m2.088626775\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m6.922596716\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.77106367\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m8.675418651\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m-0.242068655\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m7.673756466\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m3.508563011\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m))\n",
       ")\n",
       "\u001b[36mmockCoefficients\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mVector\u001b[39m(\u001b[32m-0.406605464\u001b[39m, \u001b[32m0.852573316\u001b[39m, \u001b[32m-1.104746259\u001b[39m)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mockDataset = Vector(\n",
    "    (2.7810836, 2.550537003,0),\n",
    "    (1.465489372, 2.362125076, 0),\n",
    "    (3.396561688, 4.400293529, 0),\n",
    "    (1.38807019, 1.850220317, 0),\n",
    "    (3.06407232, 3.005305973, 0),\n",
    "    (7.627531214, 2.759262235, 1),\n",
    "    (5.332441248, 2.088626775, 1),\n",
    "    (6.922596716, 1.77106367, 1),\n",
    "    (8.675418651,-0.242068655, 1),\n",
    "    (7.673756466, 3.508563011, 1)).map{ case (x1, x2, y) => Vector(Numeric(x1), Numeric(x2), Numeric(y)) }\n",
    "\n",
    "val mockCoefficients = Vector(-0.406605464, 0.852573316, -1.104746259)\n",
    "\n",
    "mockDataset.foreach { case row @ Vector(Numeric(x1), Numeric(x2), Numeric(y)) => \n",
    "    val predicted = predictLogisticRegression(row, mockCoefficients)\n",
    "    println(s\"Expected=$y, Predicted=$predicted [${math.round(predicted)}]\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Coefficients\n",
    "\n",
    "Now that we have a predicting function in place, the next step is to implement a function to estimate the coefficients that'll be used later on in the pipeline:\n",
    "\n",
    "We are implementing Stochastic Gradient Descent. It requires two parameters:\n",
    "\n",
    " - __Learning Rate__: It is used to control the amount of correction each parameter will receive at a time.\n",
    " - __Number of epochs__: Number of times the algorithm will loop over all the data, updating the coefficients.\n",
    " \n",
    "The outline of the algorithm is as follows:\n",
    "\n",
    " 1. Loop over each epoch.\n",
    " 2. Loop over each row in the training set.\n",
    " 3. Loop over each coefficient and update it for a row in an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcoefficientsLogisticRegressionSgd\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def coefficientsLogisticRegressionSgd(train: Dataset, learningRate: Double, numberOfEpochs: Int) = {\n",
    "  var coefficients = Vector.fill(train.head.length)(0.0)\n",
    "\n",
    "  for {\n",
    "    _ <- 1 to numberOfEpochs\n",
    "    row <- train\n",
    "\n",
    "  } {\n",
    "    val predicted = predictLogisticRegression(row, coefficients)\n",
    "    val actual = getNumericValue(row.last).get\n",
    "    val error = actual - predicted\n",
    "\n",
    "    val bias = coefficients.head + learningRate * error * predicted * (1.0 - predicted)\n",
    "    val indices = row.indices.init\n",
    "\n",
    "    val remainingCoefficients = indices.foldLeft(coefficients) { (c, index) =>\n",
    "      val actual = getNumericValue(row(index)).get\n",
    "      updatedVector(c, c(index + 1) + learningRate * error * predicted * (1.0 - predicted) * actual, index + 1)\n",
    "    }\n",
    "\n",
    "    coefficients = Vector(bias) ++ remainingCoefficients.tail\n",
    "  }\n",
    "\n",
    "  coefficients\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the coefficients for our mock dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres5\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mVector\u001b[39m(\u001b[32m-0.8596443546618894\u001b[39m, \u001b[32m1.5223825112460012\u001b[39m, \u001b[32m-2.2187002105650175\u001b[39m)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficientsLogisticRegressionSgd(mockDataset, 0.3, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Now that we have all the pieces, defining logistic regression is easy. Let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mlogisticRegression\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logisticRegression(train: Dataset, test: Dataset, parameters: Parameters) = {\n",
    "  val learningRate = parameters(\"learningRate\").asInstanceOf[Double]\n",
    "  val numberOfEpochs = parameters(\"numberOfEpochs\").asInstanceOf[Int]\n",
    "\n",
    "  val coefficients = coefficientsLogisticRegressionSgd(train, learningRate, numberOfEpochs)\n",
    "\n",
    "  test.map { row =>\n",
    "    Numeric(math.round(predictLogisticRegression(row, coefficients)))\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. We just need to unpack the relevant parameters, use SGD to obtain the coefficients and then use them to make predictions on the test set.\n",
    "\n",
    "Let's now use our new algorithm to test it on the Pima Indians Diabetes dataset.\n",
    "\n",
    "We'll start by running a baseline model on it and then our freshly implemented logistic regression algorithm and then we will compare their performance.\n",
    "\n",
    "As a baseline we will use a __random algorithm classifier__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Algorithm accuracy: 0.474025974025974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mminMax\u001b[39m: \u001b[32mMinMaxData\u001b[39m = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.0\u001b[39m, \u001b[32m17.0\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.0\u001b[39m, \u001b[32m199.0\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.0\u001b[39m, \u001b[32m122.0\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.0\u001b[39m, \u001b[32m99.0\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.0\u001b[39m, \u001b[32m846.0\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.0\u001b[39m, \u001b[32m67.1\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.078\u001b[39m, \u001b[32m2.42\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m21.0\u001b[39m, \u001b[32m81.0\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.0\u001b[39m, \u001b[32m1.0\u001b[39m))\n",
       ")\n",
       "\u001b[36mnormalizedData\u001b[39m: \u001b[32mDataset\u001b[39m = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\n",
       "    Numeric(0.35294117647058826),\n",
       "    Numeric(0.7437185929648241),\n",
       "    Numeric(0.5901639344262295),\n",
       "    Numeric(0.35353535353535354),\n",
       "    Numeric(0.0),\n",
       "    Numeric(0.5007451564828614),\n",
       "    Numeric(0.23441502988898377),\n",
       "    Numeric(0.48333333333333334),\n",
       "    Numeric(1.0)\n",
       "  ),\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mbaselineAccuracy\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.474025974025974\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Normalize data\n",
    "val minMax = getDatasetMinAndMax(data)\n",
    "val normalizedData = normalizeDataset(data, minMax)\n",
    "\n",
    "val baselineAccuracy = evaluateAlgorithmUsingTrainTestSplit[Numeric](\n",
    "        normalizedData, \n",
    "        (train, test, parameters) => randomAlgorithm(train, test), \n",
    "        Map.empty, \n",
    "        accuracy, \n",
    "        trainProportion=0.8)\n",
    "\n",
    "println(s\"Random Algorithm accuracy: $baselineAccuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlogisticRegressionAccuracy\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.7337662337662337\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val logisticRegressionAccuracy = evaluateAlgorithmUsingTrainTestSplit[Numeric](\n",
    "        normalizedData, \n",
    "        logisticRegression, \n",
    "        Map(\"learningRate\" -> 0.1, \"numberOfEpochs\" -> 100), \n",
    "        accuracy, \n",
    "        trainProportion=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our logistic regression algorithm performs much better than the baseline random algorithm we defined above (47.4% accuracy vs. 73.38%). \n",
    "\n",
    "We could squeeze more predictive power by tweaking the learning rate and the number of epochs. Feel free to experiment! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
