{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap Aggregation\n",
    "\n",
    "Although decision trees are a very porwerful, expresive and versatile algorihtm, they tend to suffer from high variance. This means that they pay too much attention to data, thus producing very different results based on the training examples they are fed with.\n",
    "\n",
    "One way to counter this tendency of overfitting to the data is called __Bootstrap Aggregation__ or __Bagging__, for short. Bagging is an ensemble method, which means that it is an algorithm that makes use of several predictors, combine their outputs in some way and then return a final, unified prediction. \n",
    "\n",
    "What is __bootstrap__? It is just a sample of a dataset with replacement. Put more simply, we can select a subset of training examples from a dataset, and any given row can be re-selected at any point in the future for any other subset.\n",
    "\n",
    "Then, Bagging consists of training a given number of decision trees on different _bootstraps_ of data and then combining their predictions. \n",
    "\n",
    "Given the characteristics of Bagging, it is a very useful approach when we do not have a lot of data available!\n",
    "\n",
    "Let's start our implementation by loading the code and libraries we'll need. We will build our solution on top of the ones we implemented in the [previous notebook](https://github.com/jesus-a-martinez-v/toy-ml/blob/master/src/main/scala/notebooks/decision_trees.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                                        , DecisionTrees._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.github.tototoshi::scala-csv:1.3.5`\n",
    "import $file.^.datasmarts.ml.toy.scripts.DecisionTrees, DecisionTrees._\n",
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We'll use the [Sonar](https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data) dataset. It involves the prediction of whether or not a certain object is a mine or a rock given the strength of sonar returns at various angles. It is, of course, a binary classification problem, perfect for our perceptron.\n",
    "\n",
    "\n",
    "Let's load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in dataset: 208\n",
      "Number of column in dataset: 61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mBASE_DATA_PATH\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"../../resources/data\"\u001b[39m\n",
       "\u001b[36msonarPath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"../../resources/data/16/sonar.all-data.csv\"\u001b[39m\n",
       "\u001b[36mrawData\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\n",
       "    Text(0.0200),\n",
       "    Text(0.0371),\n",
       "    Text(0.0428),\n",
       "    Text(0.0207),\n",
       "    Text(0.0954),\n",
       "    Text(0.0986),\n",
       "    Text(0.1539),\n",
       "    Text(0.1601),\n",
       "    Text(0.3109),\n",
       "    Text(0.2111),\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mnumberOfRows\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m208\u001b[39m\n",
       "\u001b[36mnumberOfColumns\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m61\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\n",
       "    Numeric(0.02),\n",
       "    Numeric(0.0371),\n",
       "    Numeric(0.0428),\n",
       "    Numeric(0.0207),\n",
       "    Numeric(0.0954),\n",
       "    Numeric(0.0986),\n",
       "    Numeric(0.1539),\n",
       "    Numeric(0.1601),\n",
       "    Numeric(0.3109),\n",
       "    Numeric(0.2111),\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mlookUpTable\u001b[39m: \u001b[32mMap\u001b[39m[\u001b[32mData\u001b[39m, \u001b[32mInt\u001b[39m] = \u001b[33mMap\u001b[39m(Text(R) -> \u001b[32m0\u001b[39m, Text(M) -> \u001b[32m1\u001b[39m)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val BASE_DATA_PATH = \"../../resources/data\"\n",
    "val sonarPath = s\"$BASE_DATA_PATH/16/sonar.all-data.csv\"\n",
    "\n",
    "val rawData = loadCsv(sonarPath)\n",
    "val numberOfRows = rawData.length\n",
    "val numberOfColumns = rawData.head.length\n",
    "println(s\"Number of rows in dataset: $numberOfRows\")\n",
    "println(s\"Number of column in dataset: $numberOfColumns\")\n",
    "\n",
    "val (data, lookUpTable) = {\n",
    "    val dataWithNumericColumns = (0 until (numberOfColumns - 1)).toVector.foldLeft(rawData) { (d, i) => textColumnToNumeric(d, i)}\n",
    "    categoricalColumnToNumeric(dataWithNumericColumns, numberOfColumns - 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Resample\n",
    "\n",
    "The first thing we need to do is implement a method to resample a subset of data from a dataset. This process can be achieved by selection a random proportion or rows of the training example. In particular, we can shuffle the dataset, determine how many rows correspond to the supplied proportion and then take that number of rows from the shuffled dataset. \n",
    "\n",
    "Let's implement a function that does this work for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msubsample\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subsample(dataset: Dataset, ratio: Double = 1.0) = {\n",
    "  val nSample = math.round(dataset.length * ratio).toInt\n",
    "\n",
    "  val shuffledDataset = Random.shuffle(dataset)\n",
    "\n",
    "  shuffledDataset.take(nSample)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. By default, the subsample will just return a permutation of the whole dataset (ratio = 1.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Given that the underlying models used in Bagging are decision trees, we just need to create a method to make predictions given a list of trained trees. We'll use each tree to make a prediction on a given row and then we'll select the mode among all predictions as the final label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mbaggingPredict\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def baggingPredict(trees: List[TreeNode], row: Vector[Numeric]): Numeric = {\n",
    "    val predictions = trees.map(t => predictWithTree(t, row))\n",
    "    predictions.maxBy(p => predictions.count(_ == p))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mbagging\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bagging(train: Dataset, test: Dataset, parameters: Parameters) = {\n",
    "  val numberOfTrees = parameters(\"numberOfTrees\").asInstanceOf[Int]\n",
    "  val maxDepth = parameters(\"maxDepth\").asInstanceOf[Int]\n",
    "  val sampleSize = parameters(\"sampleSize\").asInstanceOf[Double]\n",
    "  val minSize = parameters(\"minSize\").asInstanceOf[Int]\n",
    "  val trees = (1 to numberOfTrees).toList.map(_ => buildTree(subsample(train, sampleSize), maxDepth, minSize))\n",
    "\n",
    "  test.map { r =>\n",
    "    baggingPredict(trees, r.asInstanceOf[Vector[Numeric]])\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good.\n",
    "\n",
    "Let's now use our new algorithm to test it on the Sonar dataset.\n",
    "\n",
    "We'll start by running a baseline model on it and then our freshly implemented Bagging algorithm and then we will compare their performance. In this case, we will use 1, 5, 10, 20, 50 and 100 trees.\n",
    "\n",
    "As a baseline for classification we will use a __zero rule classifier__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Rule accuracy: 0.5714285714285714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mminMax\u001b[39m: \u001b[32mMinMaxData\u001b[39m = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.0015\u001b[39m, \u001b[32m0.1371\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m6.0E-4\u001b[39m, \u001b[32m0.2339\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.0015\u001b[39m, \u001b[32m0.3059\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.0058\u001b[39m, \u001b[32m0.4264\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.0067\u001b[39m, \u001b[32m0.401\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.0102\u001b[39m, \u001b[32m0.3823\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.0033\u001b[39m, \u001b[32m0.3729\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.0055\u001b[39m, \u001b[32m0.459\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.0075\u001b[39m, \u001b[32m0.6828\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.0113\u001b[39m, \u001b[32m0.7106\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.0289\u001b[39m, \u001b[32m0.7342\u001b[39m)),\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mnormalizedData\u001b[39m: \u001b[32mDataset\u001b[39m = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\n",
       "    Numeric(0.1364306784660767),\n",
       "    Numeric(0.15645092156022286),\n",
       "    Numeric(0.13567674113009198),\n",
       "    Numeric(0.03542558250118878),\n",
       "    Numeric(0.22495561755008875),\n",
       "    Numeric(0.2375705455522709),\n",
       "    Numeric(0.40746753246753253),\n",
       "    Numeric(0.3409040793825799),\n",
       "    Numeric(0.44928180068117873),\n",
       "    Numeric(0.2857142857142857),\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mbaselineAccuracy\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.5714285714285714\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Normalize data\n",
    "val minMax = getDatasetMinAndMax(data)\n",
    "val normalizedData = normalizeDataset(data, minMax)\n",
    "\n",
    "val baselineAccuracy = evaluateAlgorithmUsingTrainTestSplit[Numeric](\n",
    "        normalizedData, \n",
    "        (train, test, parameters) => zeroRuleClassifier(train, test), \n",
    "        Map.empty, \n",
    "        accuracy, \n",
    "        trainProportion=0.8)\n",
    "\n",
    "println(s\"Zero Rule accuracy: $baselineAccuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 trees.\n",
      "Bagging accuracy: 0.6666666666666666\n",
      "Using 5 trees.\n",
      "Bagging accuracy: 0.6666666666666666\n",
      "Using 10 trees.\n",
      "Bagging accuracy: 0.7142857142857143\n",
      "Using 20 trees.\n",
      "Bagging accuracy: 0.7380952380952381\n",
      "Using 50 trees.\n",
      "Bagging accuracy: 0.7142857142857143\n",
      "Using 100 trees.\n",
      "Bagging accuracy: 0.7380952380952381\n"
     ]
    }
   ],
   "source": [
    "for (nTrees <- List(1, 5, 10, 20, 50, 100)) {\n",
    "    println(s\"Using $nTrees trees.\")\n",
    "    val baggingAccuracy = evaluateAlgorithmUsingTrainTestSplit[Numeric](\n",
    "        data,\n",
    "        bagging,\n",
    "        Map(\"maxDepth\" -> 6, \"minSize\" -> 2, \"sampleSize\" -> 0.5, \"numberOfTrees\" -> nTrees),\n",
    "        accuracy,\n",
    "        trainProportion=0.8)\n",
    "    \n",
    "    println(s\"Bagging accuracy: $baggingAccuracy\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that even one single tree achieves better performance than the baseline defined above. The problem here is that we are building very shallow trees, thus their predictive power is not very impressive. It is only when we start combining a good amount (20 or more) of them that we start seeing the benefits of bagging!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
