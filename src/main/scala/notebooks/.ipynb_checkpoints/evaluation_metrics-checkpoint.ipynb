{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "One of the most important building blocks of machine learning are the evaluation metrics. Why? Well, even though having a computer make predictions on the data we supplied to it is _cool_, not any predictor is useful or even fit for the problem we are trying to solve or tackle using machine learning.\n",
    "\n",
    "So, how do we tell how __good__ is a certain model? Using math, of course! \n",
    "\n",
    "Enter __evaluation metrics__!\n",
    "\n",
    "An evaluation metrics, roughly speaking, is just a function that takes the prediction our model generates, compare them with the actual labels in the data, and give us a number that indicates how good (or bad) our algorithm is doing. There are many evaluation functions. Some of them are specific to regression and others to classification. Of course, each of them have a set of features that make them stand out at specific circunstances. The ones we are going to explore in this notebook are:\n",
    "\n",
    "    - Accuracy.\n",
    "    - Confusion Matrix.\n",
    "    - Mean Absolute Error (MAE).\n",
    "    - Root Mean Squared Error (RMSE).\n",
    "    - Precission.\n",
    "    - Recall.\n",
    "    - F1.\n",
    "    \n",
    "Let's get started, shall we?\n",
    "\n",
    "Let's start our implementation by loading the code and libraries we'll need. We will build our solution on top of the ones we implemented in the [previous notebook](https://github.com/jesus-a-martinez-v/toy-ml/blob/master/src/main/scala/notebooks/algorithm_evaluation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector(Vector(Numeric(5.0)), Vector(Numeric(7.0)))\n",
      "Vector(Vector(Numeric(3.0)), Vector(Numeric(2.0)))\n",
      "Vector(Vector(Numeric(8.0)), Vector(Numeric(10.0)))\n",
      "Vector(Vector(Numeric(9.0)), Vector(Numeric(6.0)))\n",
      "Vector(Vector(Numeric(4.0)), Vector(Numeric(1.0)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                                              , AlgorithmEvaluation._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.github.tototoshi::scala-csv:1.3.5`\n",
    "import $file.^.datasmarts.ml.toy.scripts.AlgorithmEvaluation, AlgorithmEvaluation._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Throughout this notebook we'll use this data to test our implementations:\n",
    "\n",
    "| ACTUAL \t|  PREDICTED  |\n",
    "|:---:\t|:----:|\n",
    "|   0  |  0  |\n",
    "|   0  |  1 |\n",
    "|   0  |  0   |\n",
    "|   0  |  0   |\n",
    "|   0  |  0   |\n",
    "|   1  |  1   |\n",
    "|   1  |  0   |\n",
    "|   1  |  1   |\n",
    "|   1  |  1   |\n",
    "|   1  |  0   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mactual\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mNumeric\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m),\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m),\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m),\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m),\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m),\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m),\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m),\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m),\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m),\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m)\n",
       ")\n",
       "\u001b[36mpredicted\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mNumeric\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m),\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m),\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m),\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m),\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m),\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m),\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m),\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m),\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m),\n",
       "  \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val actual = Vector(0, 0, 0, 0, 0, 1, 1, 1, 1, 1).map(Numeric)\n",
    "val predicted = Vector(0, 1, 0, 0, 0, 1, 0, 1, 1, 0).map(Numeric)\n",
    "\n",
    "assert(actual.length == predicted.length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Accuracy is the relation between the predictions our model got right and the total predictions it made. This is one of the most intuitive and simple evaluation metrics there are.\n",
    "\n",
    "One of its clear disadvantages is that it only tells one side of the story: The number of correct predictions. What about the predictions the algorithm failed? Aren't them important? \n",
    "\n",
    "It is also very sensitive to inbalanced datasets. In this cases, for instance, a model that always predicts the predominant label will achieve a very high accuracy, but under the hood it isn't predicting at all, just throwing some constant back!\n",
    "\n",
    "Despite its flaws, accuracy is very useful, in particular when we deal with somewhat balanced datasets and binary classification tasks. \n",
    "\n",
    "The formula for accuracy is:\n",
    "\n",
    "$$ accuracy = \\frac{\\sum_{i=1}^{N} (if\\ prediction_i\\ =\\ actual_i\\ then\\ 1\\ else\\ 0)}{N}$$\n",
    "\n",
    "Let's implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36maccuracy\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(actual: Vector[Data], predicted: Vector[Data]): Double = {\n",
    "  // We can only compate vectors of equal length\n",
    "  assert(actual.length == predicted.length)\n",
    "\n",
    "  val indices = actual.indices\n",
    "  val numberOfTotalPredictions = predicted.length\n",
    "\n",
    "  val numberOfCorrectPredictions = indices.foldLeft(0.0) { (accumulated, index) =>\n",
    "    accumulated + (if (actual(index) == predicted(index)) 1.0 else 0.0)\n",
    "  }\n",
    "\n",
    "  numberOfCorrectPredictions / numberOfTotalPredictions\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test it in our mock dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is of 70.0%\n"
     ]
    }
   ],
   "source": [
    "println(s\"Accuracy is of ${accuracy(actual, predicted) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A confusion or error matrix is just a 2x2 arrange of the performance of the algorithm on each unique class in the labels. The rows represent predicted classes and the columns the actual classes. \n",
    "\n",
    "For instance, for a binary problem with only two classes $ \\{0, 1\\} $ a confusion matrix will contain the following info (assuming $ 1 $ is the possitive class):\n",
    "\n",
    "|  \t    |  1 | 0  |\n",
    "|:---:\t|:----:| :--: |\n",
    "|   **1** \t|  True Positives  |   False Positives  |\n",
    "|   **0**  |  False Negatives  |   True Negatives  |\n",
    "\n",
    "Where:\n",
    "\n",
    "  - __True Positive__: The actual class was __1__ and the model predicted __1__.\n",
    "  - __False Positive__: The actual class was __1__ but the model predicted __0__.\n",
    "  - **False Negative**: The actual class was __0__ but the model predicted __1__.\n",
    "  - **True Negative**: The actual class was __0__ and the model predicted __0__.\n",
    "\n",
    "Let's implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mconfusionMatrix\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def confusionMatrix(actual: Vector[Data], predicted: Vector[Data], positiveLabel: Data): Map[String, Int] = {\n",
    "  assert(actual.length == predicted.length)\n",
    "\n",
    "  actual.indices.foldLeft(Map(\"TP\" -> 0, \"FP\" -> 0, \"FN\" -> 0, \"TN\" -> 0)) { (matrix, index) =>\n",
    "    val actualLabel = actual(index)\n",
    "    val predictedLabel = predicted(index)\n",
    "\n",
    "    if (actualLabel == positiveLabel) {\n",
    "      if (actualLabel == predictedLabel) {\n",
    "        matrix + (\"TP\" -> (matrix(\"TP\") + 1))\n",
    "      } else {\n",
    "        matrix + (\"FP\" -> (matrix(\"FP\") + 1))\n",
    "      }\n",
    "    } else {\n",
    "      if (actualLabel == predictedLabel) {\n",
    "        matrix + (\"TN\" -> (matrix(\"TN\") + 1))\n",
    "      } else {\n",
    "        matrix + (\"FN\" -> (matrix(\"FN\") + 1))\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function we implemented returns a map where the keys are $ \\{TP, FP, FN, TN\\}$ and correspond to $ \\{True\\ Positive,\\ False\\ Positive,\\ False\\ Negative,\\ True\\ Negative\\ \\} $. The values are just the counts for each category. \n",
    "\n",
    "We must supply the positive label in order to break ambiguity.\n",
    "\n",
    "Let's now test it in our mock dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 3\n",
      "False positives: 2\n",
      "False negatives: 2\n",
      "True negatives: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mmatrix\u001b[39m: \u001b[32mMap\u001b[39m[\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m] = \u001b[33mMap\u001b[39m(\u001b[32m\"TP\"\u001b[39m -> \u001b[32m3\u001b[39m, \u001b[32m\"FP\"\u001b[39m -> \u001b[32m2\u001b[39m, \u001b[32m\"FN\"\u001b[39m -> \u001b[32m1\u001b[39m, \u001b[32m\"TN\"\u001b[39m -> \u001b[32m4\u001b[39m)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val matrix = confusionMatrix(actual, predicted, Numeric(1.0))\n",
    "\n",
    "println(s\"True positives: ${matrix(\"TP\")}\")\n",
    "println(s\"False positives: ${matrix(\"FP\")}\")\n",
    "println(s\"False negatives: ${matrix(\"FP\")}\")\n",
    "println(s\"True negatives: ${matrix(\"TN\")}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error\n",
    "\n",
    "Mean Absolute Error (MAE) is a metric used in regression problems where our goal shifts from predicting the correct class or label to minimizing the error between the value our model outputs and the actual value. \n",
    "\n",
    "Of course, this means this metric only works with numeric data. \n",
    "\n",
    "MAE basically averages the absolute error of our algorithm. Why absolute? So we can add them up without worrying about the sign.\n",
    "\n",
    "\n",
    "The formula for MAE is:\n",
    "\n",
    "$$ MAE = \\frac{\\sum_{i=1}^{N} |value_i - actual_i|}{N}$$\n",
    "\n",
    "Let's implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmeanAbsoluteError\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def meanAbsoluteError(actual: Vector[Numeric], predicted: Vector[Numeric]): Double = {\n",
    "  assert(actual.length == predicted.length)\n",
    "\n",
    "  val sumOfAbsoluteErrors = actual.indices.foldLeft(0.0) { (accumulated, index) =>\n",
    "    accumulated + math.abs(actual(index).value - predicted(index).value)\n",
    "  }\n",
    "\n",
    "  sumOfAbsoluteErrors / actual.length\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test it in our mock dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE is 0.3"
     ]
    }
   ],
   "source": [
    "print(s\"MAE is ${meanAbsoluteError(actual, predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error\n",
    "\n",
    "Root Mean Squared Error (RMSE) is another metric used in regression problems.\n",
    "\n",
    "It is very similar to MAE and, again, it's only suited to numeric data.\n",
    "\n",
    "The main advantage of RMSE is that squaring the error forces it to be always positive and also penalizes larger errors with lower score. Also, squaring the MSE returns the value to the original units.\n",
    "\n",
    "The formula for RMSE is:\n",
    "\n",
    "$$ RMSE = \\sqrt{\\frac{\\sum_{i=1}^{N} (value_i - actual_i)^2}{N}}$$\n",
    "\n",
    "Let's implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mrootMeanSquaredError\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rootMeanSquaredError(actual: Vector[Numeric], predicted: Vector[Numeric]): Double = {\n",
    "  assert(actual.length == predicted.length)\n",
    "\n",
    "  val sumOfSquaredErrors = actual.indices.foldLeft(0.0) { (accumulated, index) =>\n",
    "    accumulated + math.pow(actual(index).value - predicted(index).value, 2)\n",
    "  }\n",
    "\n",
    "  math.sqrt(sumOfSquaredErrors / actual.length)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test it in our mock dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 0.5477225575051661"
     ]
    }
   ],
   "source": [
    "print(s\"RMSE is ${rootMeanSquaredError(actual, predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "Precision, also known as __Positive Predictive Value__ is just the ration between the true positives (the predictions of the possitive class when the actual class was also positive) and all the positives predicted by the model.\n",
    "\n",
    "$$ precision = \\frac{True\\ Positives}{True\\ Positives\\ +\\ False\\ Positives}$$\n",
    "\n",
    "It is, of course, a metric used in classification tasks.\n",
    "\n",
    "Let's implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprecision\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision(actual: Vector[Data], predicted: Vector[Data], positiveLabel: Data): Double = {\n",
    "  assert(actual.length == predicted.length)\n",
    "\n",
    "  val matrix = confusionMatrix(actual, predicted, positiveLabel)\n",
    "\n",
    "  matrix(\"TP\").toDouble / (matrix(\"TP\") + matrix(\"FP\")).toDouble\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test it in our mock dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm's precision is 0.6. This means that it has an 60.0% accuracy predicting positive labels.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mppv\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.6\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ppv = precision(actual, predicted, Numeric(1))\n",
    "println(s\"Algorithm's precision is $ppv. This means that it has an ${ppv * 100}% accuracy predicting positive labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "Recall is also another useful evaluation metric for classification problems, \n",
    "\n",
    "Recall, also known as __Sensitivity__ is the proportion of positive examples that were actually identified as such by the algorithm. \n",
    "\n",
    "The main difference with __precision__ is that the while __recall__ measures the ratio of times the model predicted a positive class when the **_actual_** class of an example was also positive, __precision__ measures the ratio of times the **_predicted_** class was positive among all the positive predictions (correct or not) of the model.\n",
    "\n",
    "$$ recall = \\frac{True\\ Positives}{True\\ Positives\\ +\\ False\\ Negatives}$$\n",
    "\n",
    "\n",
    "Let's implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mrecall\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recall(actual: Vector[Data], predicted: Vector[Data], positiveLabel: Data): Double = {\n",
    "  assert(actual.length == predicted.length)\n",
    "\n",
    "  val matrix = confusionMatrix(actual, predicted, positiveLabel)\n",
    "\n",
    "  matrix(\"TP\").toDouble / (matrix(\"TP\") + matrix(\"FN\")).toDouble\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test it in our mock dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm's recall is 0.75. This means that it has an 75.0% accuracy identifying positive labels.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msensitivity\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.75\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sensitivity = recall(actual, predicted, Numeric(1))\n",
    "println(s\"Algorithm's recall is $sensitivity. This means that it has an ${sensitivity * 100}% accuracy identifying positive labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score\n",
    "\n",
    "F1 score is the harmonical average of precision and recall. Its main advantage is that summarizes the precision and recall in a single quantity. On the flipside, it hasn't an intuitive interpretation as happens with many of the other metrics we've seen in this notebook.\n",
    "\n",
    "F1 reaches its highest at 1.0 and its lowest at 0.0, where the former means the model has a perfect predictive power (impossible) and 0.0 means the algorithm misses all the time (sad).\n",
    "\n",
    "$$ F_1 = \\frac{recall}{precision + recall}$$\n",
    "\n",
    "It is, of course, a metric used in classification tasks.\n",
    "\n",
    "Let's implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mf1\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1(actual: Vector[Data], predicted: Vector[Data], positiveLabel: Data): Double = {\n",
    "  assert(actual.length == predicted.length)\n",
    "\n",
    "  val precisionValue = precision(actual, predicted, positiveLabel)\n",
    "  val recallValue = recall(actual, predicted, positiveLabel)\n",
    "\n",
    "  (precisionValue * recallValue) / (precisionValue + recallValue)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test it in our mock dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres15\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.33333333333333326\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(actual, predicted, Numeric(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A $F_1$ score of 0.333 translates into a poor performance. Although the model is somewhat good at identifying positive instances (75% of the times), it fails at predicting them almost half of the time (60%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
