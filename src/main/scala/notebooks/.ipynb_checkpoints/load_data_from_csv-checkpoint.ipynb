{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data From CSV\n",
    "\n",
    "One of the most pervasive formats of data in Machine Learning Land is tabular. Tabular refers to data that is simply stored in a structure of rows and columns that resembles a table.\n",
    "\n",
    "Tabular data is stored in a format known as COMMA-SEPARATED-VALUES (CSV) which is fairly self explanatory. CSV files are comprised of rows, where each value is separated by commas (,). There are other common separators such as TABS and |.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a CSV file\n",
    "\n",
    "At first, loading a CSV file might seem as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows is 768 and number of columns is 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.io.Source\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mnaivelyReadCsv\u001b[39m\n",
       "\u001b[36mBASE_DATA_PATH\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"../../resources/data\"\u001b[39m\n",
       "\u001b[36mpimaIndiansPath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"../../resources/data/1/pima-indians-diabetes.csv\"\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m]] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[33mArray\u001b[39m(\u001b[32m\"6\"\u001b[39m, \u001b[32m\"148\"\u001b[39m, \u001b[32m\"72\"\u001b[39m, \u001b[32m\"35\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"33.6\"\u001b[39m, \u001b[32m\"0.627\"\u001b[39m, \u001b[32m\"50\"\u001b[39m, \u001b[32m\"1\"\u001b[39m),\n",
       "  \u001b[33mArray\u001b[39m(\u001b[32m\"1\"\u001b[39m, \u001b[32m\"85\"\u001b[39m, \u001b[32m\"66\"\u001b[39m, \u001b[32m\"29\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"26.6\"\u001b[39m, \u001b[32m\"0.351\"\u001b[39m, \u001b[32m\"31\"\u001b[39m, \u001b[32m\"0\"\u001b[39m),\n",
       "  \u001b[33mArray\u001b[39m(\u001b[32m\"8\"\u001b[39m, \u001b[32m\"183\"\u001b[39m, \u001b[32m\"64\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"23.3\"\u001b[39m, \u001b[32m\"0.672\"\u001b[39m, \u001b[32m\"32\"\u001b[39m, \u001b[32m\"1\"\u001b[39m),\n",
       "  \u001b[33mArray\u001b[39m(\u001b[32m\"1\"\u001b[39m, \u001b[32m\"89\"\u001b[39m, \u001b[32m\"66\"\u001b[39m, \u001b[32m\"23\"\u001b[39m, \u001b[32m\"94\"\u001b[39m, \u001b[32m\"28.1\"\u001b[39m, \u001b[32m\"0.167\"\u001b[39m, \u001b[32m\"21\"\u001b[39m, \u001b[32m\"0\"\u001b[39m),\n",
       "  \u001b[33mArray\u001b[39m(\u001b[32m\"0\"\u001b[39m, \u001b[32m\"137\"\u001b[39m, \u001b[32m\"40\"\u001b[39m, \u001b[32m\"35\"\u001b[39m, \u001b[32m\"168\"\u001b[39m, \u001b[32m\"43.1\"\u001b[39m, \u001b[32m\"2.288\"\u001b[39m, \u001b[32m\"33\"\u001b[39m, \u001b[32m\"1\"\u001b[39m),\n",
       "  \u001b[33mArray\u001b[39m(\u001b[32m\"5\"\u001b[39m, \u001b[32m\"116\"\u001b[39m, \u001b[32m\"74\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"25.6\"\u001b[39m, \u001b[32m\"0.201\"\u001b[39m, \u001b[32m\"30\"\u001b[39m, \u001b[32m\"0\"\u001b[39m),\n",
       "  \u001b[33mArray\u001b[39m(\u001b[32m\"3\"\u001b[39m, \u001b[32m\"78\"\u001b[39m, \u001b[32m\"50\"\u001b[39m, \u001b[32m\"32\"\u001b[39m, \u001b[32m\"88\"\u001b[39m, \u001b[32m\"31.0\"\u001b[39m, \u001b[32m\"0.248\"\u001b[39m, \u001b[32m\"26\"\u001b[39m, \u001b[32m\"1\"\u001b[39m),\n",
       "  \u001b[33mArray\u001b[39m(\u001b[32m\"10\"\u001b[39m, \u001b[32m\"115\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"35.3\"\u001b[39m, \u001b[32m\"0.134\"\u001b[39m, \u001b[32m\"29\"\u001b[39m, \u001b[32m\"0\"\u001b[39m),\n",
       "  \u001b[33mArray\u001b[39m(\u001b[32m\"2\"\u001b[39m, \u001b[32m\"197\"\u001b[39m, \u001b[32m\"70\"\u001b[39m, \u001b[32m\"45\"\u001b[39m, \u001b[32m\"543\"\u001b[39m, \u001b[32m\"30.5\"\u001b[39m, \u001b[32m\"0.158\"\u001b[39m, \u001b[32m\"53\"\u001b[39m, \u001b[32m\"1\"\u001b[39m),\n",
       "  \u001b[33mArray\u001b[39m(\u001b[32m\"8\"\u001b[39m, \u001b[32m\"125\"\u001b[39m, \u001b[32m\"96\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"0.0\"\u001b[39m, \u001b[32m\"0.232\"\u001b[39m, \u001b[32m\"54\"\u001b[39m, \u001b[32m\"1\"\u001b[39m),\n",
       "  \u001b[33mArray\u001b[39m(\u001b[32m\"4\"\u001b[39m, \u001b[32m\"110\"\u001b[39m, \u001b[32m\"92\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"37.6\"\u001b[39m, \u001b[32m\"0.191\"\u001b[39m, \u001b[32m\"30\"\u001b[39m, \u001b[32m\"0\"\u001b[39m),\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source\n",
    "\n",
    "def naivelyReadCsv(filePath: String): Array[Array[String]] = {\n",
    "    Source\n",
    "        .fromFile(filePath)\n",
    "        .getLines\n",
    "        .map(_.split(\",\"))\n",
    "        .toArray\n",
    "}\n",
    "\n",
    "val BASE_DATA_PATH = \"../../resources/data\"\n",
    "val pimaIndiansPath = s\"$BASE_DATA_PATH/1/pima-indians-diabetes.csv\"\n",
    "\n",
    "val data = naivelyReadCsv(pimaIndiansPath)\n",
    "\n",
    "println(s\"Number of rows is ${data.length} and number of columns is ${data.head.length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Fairly good, right? Apparently everything worked out well. \n",
    "\n",
    "Sure. But there are many corner cases we didn't consider, such as:\n",
    "\n",
    "   - Files with headers.\n",
    "   - Other separators such as | or TABS.\n",
    "   - Quoted values.\n",
    "   - Commas inside quoted values.\n",
    "   - Data types besides Strings.\n",
    "   - Empty lines.\n",
    "   \n",
    "As we can see, our naïve implementation falls short. The true of the matter is that implementing a CSV library from scratch is an unnecessary pain for our purpose, which is getting into the guts of several machine learning algorithms in order to obtain a deeper, better understanding of how they work.\n",
    "\n",
    "So, that being said, let's rely on [scala-csv](https://github.com/tototoshi/scala-csv) a very, very cool CSV library for Scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.github.tototoshi::scala-csv:1.3.5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try again using this handy library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows is 768 and number of columns is 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mcom.github.tototoshi.csv._\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mloadCsvAsString\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mList\u001b[39m[\u001b[32mString\u001b[39m]] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mList\u001b[39m(\u001b[32m\"6\"\u001b[39m, \u001b[32m\"148\"\u001b[39m, \u001b[32m\"72\"\u001b[39m, \u001b[32m\"35\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"33.6\"\u001b[39m, \u001b[32m\"0.627\"\u001b[39m, \u001b[32m\"50\"\u001b[39m, \u001b[32m\"1\"\u001b[39m),\n",
       "  \u001b[33mList\u001b[39m(\u001b[32m\"1\"\u001b[39m, \u001b[32m\"85\"\u001b[39m, \u001b[32m\"66\"\u001b[39m, \u001b[32m\"29\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"26.6\"\u001b[39m, \u001b[32m\"0.351\"\u001b[39m, \u001b[32m\"31\"\u001b[39m, \u001b[32m\"0\"\u001b[39m),\n",
       "  \u001b[33mList\u001b[39m(\u001b[32m\"8\"\u001b[39m, \u001b[32m\"183\"\u001b[39m, \u001b[32m\"64\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"23.3\"\u001b[39m, \u001b[32m\"0.672\"\u001b[39m, \u001b[32m\"32\"\u001b[39m, \u001b[32m\"1\"\u001b[39m),\n",
       "  \u001b[33mList\u001b[39m(\u001b[32m\"1\"\u001b[39m, \u001b[32m\"89\"\u001b[39m, \u001b[32m\"66\"\u001b[39m, \u001b[32m\"23\"\u001b[39m, \u001b[32m\"94\"\u001b[39m, \u001b[32m\"28.1\"\u001b[39m, \u001b[32m\"0.167\"\u001b[39m, \u001b[32m\"21\"\u001b[39m, \u001b[32m\"0\"\u001b[39m),\n",
       "  \u001b[33mList\u001b[39m(\u001b[32m\"0\"\u001b[39m, \u001b[32m\"137\"\u001b[39m, \u001b[32m\"40\"\u001b[39m, \u001b[32m\"35\"\u001b[39m, \u001b[32m\"168\"\u001b[39m, \u001b[32m\"43.1\"\u001b[39m, \u001b[32m\"2.288\"\u001b[39m, \u001b[32m\"33\"\u001b[39m, \u001b[32m\"1\"\u001b[39m),\n",
       "  \u001b[33mList\u001b[39m(\u001b[32m\"5\"\u001b[39m, \u001b[32m\"116\"\u001b[39m, \u001b[32m\"74\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"25.6\"\u001b[39m, \u001b[32m\"0.201\"\u001b[39m, \u001b[32m\"30\"\u001b[39m, \u001b[32m\"0\"\u001b[39m),\n",
       "  \u001b[33mList\u001b[39m(\u001b[32m\"3\"\u001b[39m, \u001b[32m\"78\"\u001b[39m, \u001b[32m\"50\"\u001b[39m, \u001b[32m\"32\"\u001b[39m, \u001b[32m\"88\"\u001b[39m, \u001b[32m\"31.0\"\u001b[39m, \u001b[32m\"0.248\"\u001b[39m, \u001b[32m\"26\"\u001b[39m, \u001b[32m\"1\"\u001b[39m),\n",
       "  \u001b[33mList\u001b[39m(\u001b[32m\"10\"\u001b[39m, \u001b[32m\"115\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"35.3\"\u001b[39m, \u001b[32m\"0.134\"\u001b[39m, \u001b[32m\"29\"\u001b[39m, \u001b[32m\"0\"\u001b[39m),\n",
       "  \u001b[33mList\u001b[39m(\u001b[32m\"2\"\u001b[39m, \u001b[32m\"197\"\u001b[39m, \u001b[32m\"70\"\u001b[39m, \u001b[32m\"45\"\u001b[39m, \u001b[32m\"543\"\u001b[39m, \u001b[32m\"30.5\"\u001b[39m, \u001b[32m\"0.158\"\u001b[39m, \u001b[32m\"53\"\u001b[39m, \u001b[32m\"1\"\u001b[39m),\n",
       "  \u001b[33mList\u001b[39m(\u001b[32m\"8\"\u001b[39m, \u001b[32m\"125\"\u001b[39m, \u001b[32m\"96\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"0.0\"\u001b[39m, \u001b[32m\"0.232\"\u001b[39m, \u001b[32m\"54\"\u001b[39m, \u001b[32m\"1\"\u001b[39m),\n",
       "  \u001b[33mList\u001b[39m(\u001b[32m\"4\"\u001b[39m, \u001b[32m\"110\"\u001b[39m, \u001b[32m\"92\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"0\"\u001b[39m, \u001b[32m\"37.6\"\u001b[39m, \u001b[32m\"0.191\"\u001b[39m, \u001b[32m\"30\"\u001b[39m, \u001b[32m\"0\"\u001b[39m),\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.github.tototoshi.csv._\n",
    "\n",
    "def loadCsvAsString(filePath: String): List[List[String]] = {\n",
    "    CSVReader.open(filePath).all()\n",
    "}\n",
    "\n",
    "val data = loadCsvAsString(pimaIndiansPath)\n",
    "\n",
    "println(s\"Number of rows is ${data.length} and number of columns is ${data.head.length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, things still work! The main difference between our naïve implementation and this is that scala-csv opts for returning a list of lists, which is a data structure less performant than arrays for random access of elements.\n",
    "\n",
    "I know, I know... Not too impressive. The good thing about this library is that we can define several parameters such as separators, quoted values and such for our reader like this:\n",
    "\n",
    "```\n",
    "implicit object MyFormat extends DefaultCSVFormat {\n",
    "  override val delimiter = '#'\n",
    "}\n",
    "```\n",
    "\n",
    "If you want to know more about scala-csv and how to tweak it, just go to their GitHub page :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Strings to Doubles\n",
    "\n",
    "We can see that all of our data in the `pima-indians-diabetes.csv` dataset is numeric, so it doesn't really makes sense to have them stored as strings. If we think about it, there are only two possible data types we are interested in: Text and Numeric. For convenience, let's choose Double as our default numeric value. Let's create a Data trait to represent this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtrait\u001b[39m \u001b[36mData\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mNumeric\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mText\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sealed trait Data\n",
    "\n",
    "case class Numeric(value: Double) extends Data\n",
    "case class Text(value: String) extends Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows is 768 and number of columns is 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mloadCsv\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\n",
       "    Text(6),\n",
       "    Text(148),\n",
       "    Text(72),\n",
       "    Text(35),\n",
       "    Text(0),\n",
       "    Text(33.6),\n",
       "    Text(0.627),\n",
       "    Text(50),\n",
       "    Text(1)\n",
       "  ),\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loadCsv(filePath: String): Vector[Vector[Data]] = {\n",
    "    val reader = CSVReader.open(filePath)\n",
    "    \n",
    "    reader\n",
    "        .toStream\n",
    "        .map(x => x.toArray.map(Text(_)).toVector)\n",
    "        .toVector\n",
    "}\n",
    "\n",
    "val data = loadCsv(pimaIndiansPath)\n",
    "\n",
    "println(s\"Number of rows is ${data.length} and number of columns is ${data.head.length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we abstracted the data, we can create a function to convert text columns to numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector(Numeric(6.0), Text(148), Text(72), Text(35), Text(0), Text(33.6), Text(0.627), Text(50), Text(1))\n",
      "Vector(Numeric(1.0), Text(85), Text(66), Text(29), Text(0), Text(26.6), Text(0.351), Text(31), Text(0))\n",
      "Vector(Numeric(8.0), Text(183), Text(64), Text(0), Text(0), Text(23.3), Text(0.672), Text(32), Text(1))\n",
      "Vector(Numeric(1.0), Text(89), Text(66), Text(23), Text(94), Text(28.1), Text(0.167), Text(21), Text(0))\n",
      "Vector(Numeric(0.0), Text(137), Text(40), Text(35), Text(168), Text(43.1), Text(2.288), Text(33), Text(1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtextColumnToNumeric\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def textColumnToNumeric(data: Vector[Vector[Data]], columnIndex: Int) = {\n",
    "    data.map { row => \n",
    "        val (firstHalf, secondHalf) = row.splitAt(columnIndex)\n",
    "        val affectedValue = \n",
    "            secondHalf.head match { \n",
    "                case Text(value) => Numeric(value.toDouble) \n",
    "                case d => d\n",
    "            }\n",
    "    \n",
    "        firstHalf ++ Vector(affectedValue) ++ secondHalf.tail\n",
    "    }\n",
    "}\n",
    "\n",
    "textColumnToNumeric(data, 0).take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the first five rows of our dataset have their first column converted to numeric values. If we want to be absolutely sure, let's run the following predicate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcolumnIsNumeric\u001b[39m: \u001b[32mBoolean\u001b[39m = \u001b[32mtrue\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val columnIsNumeric = textColumnToNumeric(data, 0).forall {\n",
    "    _.head match {\n",
    "        case _: Numeric => true \n",
    "        case _ => false\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Categories to Numbers\n",
    "\n",
    "Now we're able to combine text and numeric data in the same data structure. Awesome! \n",
    "\n",
    "However, there are some algorithms that doesn't feel quite confortable handling data of diverse types. In fact, they prefer all their data to be numeric. So, for instance, if we have three animal categories such as `Bird`, `Dog`, `Cat`, a model would prefer instead 0, 1 and 2 (or any other three numbers).\n",
    "\n",
    "Our second dataset (located in `resources/data/1/iris.csv`) has a categorical column. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text(5.1), Text(3.5), Text(1.4), Text(0.2), Text(Iris-setosa)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mirisDataPath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"../../resources/data/1/iris.csv\"\u001b[39m\n",
       "\u001b[36mirisData\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(Text(5.1), Text(3.5), Text(1.4), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.9), Text(3.0), Text(1.4), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.7), Text(3.2), Text(1.3), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.6), Text(3.1), Text(1.5), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.0), Text(3.6), Text(1.4), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.4), Text(3.9), Text(1.7), Text(0.4), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.6), Text(3.4), Text(1.4), Text(0.3), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.0), Text(3.4), Text(1.5), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.4), Text(2.9), Text(1.4), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.9), Text(3.1), Text(1.5), Text(0.1), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.4), Text(3.7), Text(1.5), Text(0.2), Text(Iris-setosa)),\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val irisDataPath = s\"$BASE_DATA_PATH/1/iris.csv\"\n",
    "val irisData = loadCsv(irisDataPath)\n",
    "\n",
    "println(irisData.head.mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to convert categorical text columns to discrete numeric values.\n",
    "\n",
    "We will do this by creating a lookup table of categories, where each of them will act as a key and the value will be a unique number that corresponds to that category. In order to revert the transformation, this function will return both the modified dataset and the lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector(Text(5.7), Text(3.0), Text(4.2), Text(1.2), Numeric(1.0))\n",
      "Vector(Text(5.0), Text(3.4), Text(1.5), Text(0.2), Numeric(0.0))\n",
      "Vector(Text(6.7), Text(3.1), Text(4.4), Text(1.4), Numeric(1.0))\n",
      "Vector(Text(5.6), Text(2.5), Text(3.9), Text(1.1), Numeric(1.0))\n",
      "Vector(Text(5.7), Text(2.9), Text(4.2), Text(1.3), Numeric(1.0))\n",
      "Vector(Text(5.6), Text(2.9), Text(3.6), Text(1.3), Numeric(1.0))\n",
      "Vector(Text(5.2), Text(2.7), Text(3.9), Text(1.4), Numeric(1.0))\n",
      "Vector(Text(6.8), Text(2.8), Text(4.8), Text(1.4), Numeric(1.0))\n",
      "Vector(Text(6.7), Text(3.3), Text(5.7), Text(2.5), Numeric(2.0))\n",
      "Vector(Text(5.4), Text(3.7), Text(1.5), Text(0.2), Numeric(0.0))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcategoricColumnToNumeric\u001b[39m\n",
       "\u001b[36mtransformedData\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(Text(5.1), Text(3.5), Text(1.4), Text(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.9), Text(3.0), Text(1.4), Text(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.7), Text(3.2), Text(1.3), Text(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.6), Text(3.1), Text(1.5), Text(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.0), Text(3.6), Text(1.4), Text(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.4), Text(3.9), Text(1.7), Text(0.4), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.6), Text(3.4), Text(1.4), Text(0.3), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.0), Text(3.4), Text(1.5), Text(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.4), Text(2.9), Text(1.4), Text(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.9), Text(3.1), Text(1.5), Text(0.1), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.4), Text(3.7), Text(1.5), Text(0.2), Numeric(0.0)),\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mlookupTable\u001b[39m: \u001b[32mMap\u001b[39m[\u001b[32mData\u001b[39m, \u001b[32mInt\u001b[39m] = \u001b[33mMap\u001b[39m(\n",
       "  Text(Iris-setosa) -> \u001b[32m0\u001b[39m,\n",
       "  Text(Iris-versicolor) -> \u001b[32m1\u001b[39m,\n",
       "  Text(Iris-virginica) -> \u001b[32m2\u001b[39m\n",
       ")\n",
       "\u001b[36msampleSize\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "\u001b[36msample\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(Text(5.7), Text(3.0), Text(4.2), Text(1.2), Numeric(1.0)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.0), Text(3.4), Text(1.5), Text(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Text(6.7), Text(3.1), Text(4.4), Text(1.4), Numeric(1.0)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.6), Text(2.5), Text(3.9), Text(1.1), Numeric(1.0)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.7), Text(2.9), Text(4.2), Text(1.3), Numeric(1.0)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.6), Text(2.9), Text(3.6), Text(1.3), Numeric(1.0)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.2), Text(2.7), Text(3.9), Text(1.4), Numeric(1.0)),\n",
       "  \u001b[33mVector\u001b[39m(Text(6.8), Text(2.8), Text(4.8), Text(1.4), Numeric(1.0)),\n",
       "  \u001b[33mVector\u001b[39m(Text(6.7), Text(3.3), Text(5.7), Text(2.5), Numeric(2.0)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.4), Text(3.7), Text(1.5), Text(0.2), Numeric(0.0))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categoricColumnToNumeric(data: Vector[Vector[Data]], columnIndex: Int) = {\n",
    "    val uniqueColumnValues = data.foldLeft(Set[Data]()) { (set, row) => \n",
    "        set + row(columnIndex) \n",
    "    }\n",
    "    \n",
    "    val lookUpTable = uniqueColumnValues.zipWithIndex.toMap\n",
    "    \n",
    "    val categorizedData = data.map { row => \n",
    "        val (firstHalf, secondHalf) = row.splitAt(columnIndex)\n",
    "        val affectedValue = Numeric(lookUpTable(secondHalf.head).toDouble)\n",
    "    \n",
    "        firstHalf ++ Vector(affectedValue) ++ secondHalf.tail\n",
    "    }\n",
    "    \n",
    "    (categorizedData, lookUpTable)\n",
    "}\n",
    "\n",
    "val (transformedData, lookupTable) = categoricColumnToNumeric(irisData, 4)\n",
    "val sampleSize = 10\n",
    "val sample = scala.util.Random.shuffle(transformedData).take(sampleSize)\n",
    "\n",
    "sample.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the output above, our function created the following mapping table:\n",
    "\n",
    "```\n",
    "Map(\n",
    "  Text(Iris-setosa) -> 0,\n",
    "  Text(Iris-versicolor) -> 1,\n",
    "  Text(Iris-virginica) -> 2\n",
    ")\n",
    "```\n",
    "\n",
    "It is possible that different runs of the function associate the categories to different numbers. That's OK. As long as each occurrence of the category gets mapped to the same number, there's no problem ;)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
