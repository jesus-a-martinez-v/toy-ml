{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Test Harnesses\n",
    "\n",
    "It is difficult (almost impossible) to know beforehand which algorithm will suit best our problem. That's why it is a (_very_) good idea to implement a machine learning test harness that we can use repeatedly and effectively to measure the performance of a particular algorithm.\n",
    "\n",
    "In this notebook we will create two algorithm test harnesses using:\n",
    "\n",
    "   - Train-test split.\n",
    "   - K-Fold Cross-Validation.\n",
    "    \n",
    "A test harness is comprised of three key building blocks:\n",
    "\n",
    "   1. A resampling method.\n",
    "   2. The algorithm to test.\n",
    "   3. The evaluation metric used to measure the performance of the algorithm.\n",
    "    \n",
    "Let's start our implementation by loading the code and libraries we'll need. We will build our solution on top of the ones we implemented in the [previous notebook](https://github.com/jesus-a-martinez-v/toy-ml/blob/master/src/main/scala/notebooks/baseline_models.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                                         , BaselineModels._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.github.tototoshi::scala-csv:1.3.5`\n",
    "import $file.^.datasmarts.ml.toy.scripts.BaselineModels, BaselineModels._\n",
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We'll use the [Pima Indians Diabetes](https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes) dataset to test our harnesses. Let's load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in dataset: 768\n",
      "Number of column in dataset: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mBASE_DATA_PATH\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"../../resources/data\"\u001b[39m\n",
       "\u001b[36mpimaIndiansPath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"../../resources/data/6/pima-indians-diabetes.csv\"\u001b[39m\n",
       "\u001b[36mrawData\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\n",
       "    Text(6),\n",
       "    Text(148),\n",
       "    Text(72),\n",
       "    Text(35),\n",
       "    Text(0),\n",
       "    Text(33.6),\n",
       "    Text(0.627),\n",
       "    Text(50),\n",
       "    Text(1)\n",
       "  ),\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mnumberOfRows\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m768\u001b[39m\n",
       "\u001b[36mnumberOfColumns\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m9\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\n",
       "    Numeric(6.0),\n",
       "    Numeric(148.0),\n",
       "    Numeric(72.0),\n",
       "    Numeric(35.0),\n",
       "    Numeric(0.0),\n",
       "    Numeric(33.6),\n",
       "    Numeric(0.627),\n",
       "    Numeric(50.0),\n",
       "    Numeric(1.0)\n",
       "  ),\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val BASE_DATA_PATH = \"../../resources/data\"\n",
    "val pimaIndiansPath = s\"$BASE_DATA_PATH/6/pima-indians-diabetes.csv\"\n",
    "\n",
    "val rawData = loadCsv(pimaIndiansPath)\n",
    "val numberOfRows = rawData.length\n",
    "val numberOfColumns = rawData.head.length\n",
    "println(s\"Number of rows in dataset: $numberOfRows\")\n",
    "println(s\"Number of column in dataset: $numberOfColumns\")\n",
    "\n",
    "val data = (0 until numberOfColumns).toVector.foldLeft(rawData) { (d, i) => textColumnToNumeric(d, i)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harnesses Assumptions\n",
    "\n",
    "Our test harnesses will receive an algorithm to evaluate and an evaluation metric to measure its performance. An algorithm is just a function that takes a __train__ and a __test__ set, as well as optional __parameters__. Let's create the proper types to represent an algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mParameters\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mOutput\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mAlgorithm\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type Parameters = Map[String, Any]\n",
    "type Output = Vector[Data]\n",
    "type Algorithm = (Dataset, Dataset, Parameters) => Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. A few remarks before moving on:\n",
    "\n",
    "   - We decided to represent the parameters as a map of Strings to Any, where the key is the name of the parameter in camelCase, and the value must be properly casted by the algorithm. If an algorithm do not require any additional parameters, just passing `Map.empty` is enough.\n",
    "   - Output is just an alias of Vector[Data] used to enhance readibility. Each algorithm takes a train and a test set and outputs the predictions made on the latter.\n",
    "    \n",
    "Let's now create a type to represent an evaluation metric:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mEvaluationMetric\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type EvaluationMetric[T <: Data] = (Vector[T], Vector[T]) => Double"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An evaluation metric is a function that takes a vector of actual results and a vector of predictions, then use them to compute some measure that is represented as a double value. By stating that an EvaluationMetric works on any type `T` that's a subclass of `Data` we ensure that it can both represent regression and classification metrics.\n",
    "\n",
    "Good. Let's now proceed to implement our first test harness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Algorithm Test Harness\n",
    "\n",
    "As it name suggests, this harness utilizes a train-test split under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mevaluateAlgorithmUsingTrainTestSplit\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluateAlgorithmUsingTrainTestSplit[T <: Data](\n",
    "    dataset: Dataset, \n",
    "    algorithm: Algorithm, \n",
    "    parameters: Parameters, \n",
    "    evaluationMetric: EvaluationMetric[T], \n",
    "    trainProportion: Double = 0.8, \n",
    "    randomSeed: Int = 42): Double = {\n",
    "  val (train, test) = trainTestSplit(dataset, trainProportion, randomSeed)\n",
    "  val predicted = algorithm(train, test, parameters).asInstanceOf[Vector[T]]\n",
    "  val actual = selectColumn(test, test.head.length - 1).asInstanceOf[Vector[T]]\n",
    "  \n",
    "  evaluationMetric(actual, predicted)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres5\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.6168831168831169\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateAlgorithmUsingTrainTestSplit(data, (train, test, parameters) => zeroRuleClassifier(train, test), Map.empty, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres6\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.4880203360256279\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateAlgorithmUsingTrainTestSplit(data, (train, test, parameters) => zeroRuleRegressor(train, test), Map.empty, rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. It works in both cases (classification and regression).\n",
    "\n",
    "Given we haven't implement any algorithm yet, we are working with our baseline models that we implemented last week. You might have noticed that we wrapped both the `zeroRuleRegressor` and `zeroRuleClassifier` in a function. That's because these algorithms don't take parameters, but our `Algorithm` type does, so we just receive the parameters and ignore them in this case.\n",
    "\n",
    "Let's now implement an algorithm test harness using K-Fold Cross-Validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation Algorithm Test Harness\n",
    "\n",
    "As it name suggests, this harness utilizes a K-Fold Cross-Validation split under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mevaluateAlgorithmUsingCrossValidation\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluateAlgorithmUsingCrossValidation[T <: Data](\n",
    "    dataset: Dataset, \n",
    "    algorithm: Algorithm, \n",
    "    parameters: Parameters, \n",
    "    evaluationMetric: EvaluationMetric[T],\n",
    "    numberOfFolds: Int = 3, \n",
    "    randomSeed: Int = 42) = {\n",
    "  val folds = crossValidationSplit(dataset, numberOfFolds, randomSeed)\n",
    "\n",
    "  for {\n",
    "    fold <- folds\n",
    "    train = folds.filterNot(_ == fold).flatten  // All but the current fold will comprise the test set\n",
    "    test = fold\n",
    "  } yield {\n",
    "    val predicted = algorithm(train, test, parameters).asInstanceOf[Vector[T]]\n",
    "    val actual = selectColumn(test, test.head.length - 1).asInstanceOf[Vector[T]]\n",
    "    evaluationMetric(actual, predicted)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36maccuracies\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mVector\u001b[39m(\u001b[32m0.640625\u001b[39m, \u001b[32m0.64453125\u001b[39m, \u001b[32m0.66796875\u001b[39m)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accuracies = evaluateAlgorithmUsingCrossValidation(data, (train, test, parameters) => zeroRuleClassifier(train, test), Map.empty, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrmses\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mVector\u001b[39m(\u001b[32m0.480071609241788\u001b[39m, \u001b[32m0.47875472342847764\u001b[39m, \u001b[32m0.47162610494047946\u001b[39m)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rmses = evaluateAlgorithmUsingCrossValidation(data, (train, test, parameters) => zeroRuleRegressor(train, test), Map.empty, rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. As we can see, this second test harness returns us the evaluation metric value for each of the folds. In order to have a unique value, we should average these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36maccuracy\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.6510416666666666\u001b[39m\n",
       "\u001b[36mrmse\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.4768174792035817\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accuracy = accuracies.sum / accuracies.length\n",
    "val rmse = rmses.sum / rmses.length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
