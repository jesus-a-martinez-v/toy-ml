{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Evaluation\n",
    "\n",
    "Our goal when we train a predictive model is to test it against new data. Unfortunately, we don't have access to unseen, fresh data at training time, so we must hold-out a portion of our training set and treat it as new data to truly measure the performance of our algorithm.\n",
    "\n",
    "These methods of using available data to estimate performance are known as resampling methods, because, as their name states, they resample our existing data. There are many of them such as:\n",
    "\n",
    "    - Train-test split.\n",
    "    - K-fold cross-validation split.\n",
    "    - Leave One Out Cross Validation.\n",
    "    - Stratification.\n",
    "    \n",
    "In this notebook we will focus on the first two. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test split\n",
    "\n",
    "This is the easiest sampling method. It splits the data in two portions according to a given percentage or proportion. It is common to use a 80%-20% split, which means that 80% of the data will be used during the training phase, while the remaining 20% will be held-out as a validation or testing set.\n",
    "\n",
    "These proportions tend to be very sensitive to the problem at hand and the volume of data available. For instance, relatively small datasets that have a row count of hundreds or thousands tend to use an 80%-20% split, while problems that require much more data, around hundreds of thousands or even million of examples tend to use splits similar to 99%-1%.\n",
    "\n",
    "In order to achieve the described split we must randomize our dataset so we don't introduce any ordering bias to the model. It is also desirable to seed the split algorithm so our results are reproducible and a fair comparison between many models can be made using the same training and testing set.\n",
    "\n",
    "Good, let's start our implementation by loading the code and libraries we'll need. We will build our solution on top of the ones we implemented in the [previous notebook](https://github.com/jesus-a-martinez-v/toy-ml/blob/master/src/main/scala/notebooks/scale_machine_learning_data.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling ScaleData.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                                    , ScaleData._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.github.tototoshi::scala-csv:1.3.5`\n",
    "import $file.^.datasmarts.ml.toy.scripts.ScaleData, ScaleData._\n",
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a simple dataset comprised of 10 rows of only one column with numbers from 1 to 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdataset\u001b[39m: \u001b[32mDataset\u001b[39m = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(Numeric(1.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(2.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(3.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(4.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(5.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(6.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(7.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(8.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(9.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(10.0))\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataset: Dataset = (1 to 10).map(number => Vector(Numeric(number))).toVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtrainTestSplit\u001b[39m\n",
       "\u001b[36mtrain\u001b[39m: \u001b[32mDataset\u001b[39m = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(Numeric(5.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(7.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(3.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(2.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(8.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(10.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(9.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(6.0))\n",
       ")\n",
       "\u001b[36mtest\u001b[39m: \u001b[32mDataset\u001b[39m = \u001b[33mVector\u001b[39m(\u001b[33mVector\u001b[39m(Numeric(4.0)), \u001b[33mVector\u001b[39m(Numeric(1.0)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trainTestSplit(dataset: Dataset, trainProportion: Double = 0.8, seed: Int = 42): (Dataset, Dataset) = {\n",
    "  val splitPoint = (dataset.length * trainProportion).toInt\n",
    "  val random = new Random(seed)\n",
    "  val shuffledDataset = random.shuffle(dataset)\n",
    "\n",
    "  shuffledDataset.splitAt(splitPoint)\n",
    "}\n",
    "\n",
    "val (train, test) = trainTestSplit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, 80% of the 10 rows (i.e., 8) correspond to the train set, while 20% form the test set.\n",
    "\n",
    "The implementation is very straightforward: First, we must determine the number of rows that correspond to the training set by multiplying the provided value in `trainProportion` by the size of the dataset (the remaining rows will comprise the test set). This value will be the index at we will split the dataset; all the rows before this index will comprise the training set and all the rows after will correspond to the test set. Then, we shuffle the rows in `dataset` and split it at the previously calculated split point.\n",
    "\n",
    "As you can see, we are using 80% (or 0.8) as the default training proportion and a seed of 42."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation\n",
    "\n",
    "One disadvantage of a simple train-test split is that we get a noisy estimate of the algorithm's performance. Instead, we can have a bigger bang for our buck by using a k-fold cross-validation approach, where instead of splitting the data in two chunks, we split it in **_k_**. Then, we will use **_k-1_** folds to train the model and the remaining fold as test set. The beauty of this method is that we will repeat it **_k_** times, so each fold will have the chance to serve as test set and all the folds will eventually be used at the training phase. Finally, to estimate the overall performance of the model, we take the average of each of the **_k_** runs.\n",
    "\n",
    "The size of each fold is determined using the following formula:\n",
    "\n",
    "$$ size = \\frac{count(dataset)}{K}  $$\n",
    "\n",
    "If `size` is not an integer, of course some of the rows in the dataset will be left out and some folds will be smaller than the other ones.\n",
    "\n",
    "Let's proceed to implement our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcrossValidationSplit\u001b[39m\n",
       "\u001b[36mfolds\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mDataset\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mVector\u001b[39m(Numeric(5.0)), \u001b[33mVector\u001b[39m(Numeric(7.0))),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mVector\u001b[39m(Numeric(3.0)), \u001b[33mVector\u001b[39m(Numeric(2.0))),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mVector\u001b[39m(Numeric(8.0)), \u001b[33mVector\u001b[39m(Numeric(10.0))),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mVector\u001b[39m(Numeric(9.0)), \u001b[33mVector\u001b[39m(Numeric(6.0))),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mVector\u001b[39m(Numeric(4.0)), \u001b[33mVector\u001b[39m(Numeric(1.0)))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def crossValidationSplit(dataset: Dataset, numberOfFolds: Int = 3, seed: Int = 42): Vector[Dataset] = {\n",
    "  val foldSize = dataset.length / numberOfFolds\n",
    "  val random = new Random(seed)\n",
    "  val shuffledDataset = random.shuffle(dataset)\n",
    "\n",
    "  shuffledDataset.grouped(foldSize).toVector\n",
    "}\n",
    "\n",
    "val folds = crossValidationSplit(dataset, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we have 4 folds of 2 rows each, and 2 of the rows were left out as part of the remainder of 10 / 4.\n",
    "\n",
    "This algorithm's implementation is pretty similar to the one we implemented before. First, we determine the fold size by dividing the number of rows in the dataset (`dataset.length`) by `numberOfFolds`. Then, we shuffle our dataset to prevent ordering bias, and finally we group our data in folds of `foldSize`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which method should we choose?\n",
    "\n",
    "The strongest argument in favor of k-fold cross-validation is that it is much more robust than a simple train-test split method because it trains K different models versus only one trained by the latter. Nevertheless, this fact means that it can be very time consuming on large datasets or when we want to evaluate a model that takes a long time to train, like a deep neural network.\n",
    "\n",
    "With the rise of deep learning and the incrementing availability of huge datasets, train-test split has gained popularity in the community because it gives a quick estimate of a model's performance due to the fact that it only trains a model. Also, in a dataset that is comprised of hundreds of thousands or even millions of examples, a simple split technique like this produces datasets with nearly identical statistical properties. Hence, the use of a k-fold cross-validation approach loses importance and practicality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
