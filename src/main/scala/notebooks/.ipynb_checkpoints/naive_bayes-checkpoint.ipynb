{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naïve Bayes\n",
    "\n",
    "What a fun name for a classifier, don't you think? :)\n",
    "\n",
    "Naïve Bayes, simply put, refer to a family of algorithms that apply the Bayes' theorem with the _naïve_ assumption that all features are independent from each other (Hence the name). \n",
    "\n",
    "Although there are (very) rare situations where all features are independent, most of the time there are at least some degree of correlation between two or more of them. For instance, one could infer that an object that is a fruit, is red, has a somewhat round shape and grows in trees is most likely an apple. Why? Because each feature gives us more information about the others and that increases our confidence in the output. Nonetheless, Naïve Bayes algorithm make the strong assumption that in any instance this correlation occurs.\n",
    "\n",
    "It doesn't sound like an accurate algorithm, right? Well, the thing is that in real life, despite this naïvety, it works really well and it also facilitates the computation __a lot__.\n",
    "\n",
    "Bayes' Theorem gives us a way to calculate the probability of a piece of data belonging to a particular class, given our prior knowledge.\n",
    "\n",
    "$$ P(class|data)=\\frac{P(data|class)*P(class)}{P(data)} $$\n",
    "\n",
    "It is somewhat confusing, but what this formula says is \"The posterior probability of the class given the data is defined as the prior probability of that data given the class by the likelihood of the class divided by the evidence of the data\". So much clearer now! (Fingers crossed)\n",
    "\n",
    "A nicer way to represent this formula is:\n",
    "\n",
    "$$ posterior=\\frac{prior*likelihood}{evidence} $$\n",
    "\n",
    "Let's start our implementation by loading the code and libraries we'll need. We will build our solution on top of the ones we implemented in the [previous notebook](https://github.com/jesus-a-martinez-v/toy-ml/blob/master/src/main/scala/notebooks/lperceptron.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                                     , Perceptron._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.github.tototoshi::scala-csv:1.3.5`\n",
    "import $file.^.datasmarts.ml.toy.scripts.Perceptron, Perceptron._\n",
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We'll use the [Iris Flower](http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data) dataset. It involves the prediction of flower species given measurements of iris flowers. \n",
    "\n",
    "Let's load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in dataset: 150\n",
      "Number of columns in dataset: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mBASE_DATA_PATH\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"../../resources/data\"\u001b[39m\n",
       "\u001b[36mirisPath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"../../resources/data/12/iris.csv\"\u001b[39m\n",
       "\u001b[36mrawData\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(Text(5.1), Text(3.5), Text(1.4), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.9), Text(3.0), Text(1.4), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.7), Text(3.2), Text(1.3), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.6), Text(3.1), Text(1.5), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.0), Text(3.6), Text(1.4), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.4), Text(3.9), Text(1.7), Text(0.4), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.6), Text(3.4), Text(1.4), Text(0.3), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.0), Text(3.4), Text(1.5), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.4), Text(2.9), Text(1.4), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.9), Text(3.1), Text(1.5), Text(0.1), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.4), Text(3.7), Text(1.5), Text(0.2), Text(Iris-setosa)),\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mnumberOfRows\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m150\u001b[39m\n",
       "\u001b[36mnumberOfColumns\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m5\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(Numeric(5.1), Numeric(3.5), Numeric(1.4), Numeric(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(4.9), Numeric(3.0), Numeric(1.4), Numeric(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(4.7), Numeric(3.2), Numeric(1.3), Numeric(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(4.6), Numeric(3.1), Numeric(1.5), Numeric(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(5.0), Numeric(3.6), Numeric(1.4), Numeric(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(5.4), Numeric(3.9), Numeric(1.7), Numeric(0.4), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(4.6), Numeric(3.4), Numeric(1.4), Numeric(0.3), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(5.0), Numeric(3.4), Numeric(1.5), Numeric(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(4.4), Numeric(2.9), Numeric(1.4), Numeric(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(4.9), Numeric(3.1), Numeric(1.5), Numeric(0.1), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(5.4), Numeric(3.7), Numeric(1.5), Numeric(0.2), Numeric(0.0)),\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mlookUpTable\u001b[39m: \u001b[32mMap\u001b[39m[\u001b[32mData\u001b[39m, \u001b[32mInt\u001b[39m] = \u001b[33mMap\u001b[39m(\n",
       "  Text(Iris-setosa) -> \u001b[32m0\u001b[39m,\n",
       "  Text(Iris-versicolor) -> \u001b[32m1\u001b[39m,\n",
       "  Text(Iris-virginica) -> \u001b[32m2\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val BASE_DATA_PATH = \"../../resources/data\"\n",
    "val irisPath = s\"$BASE_DATA_PATH/12/iris.csv\"\n",
    "\n",
    "val rawData = loadCsv(irisPath)\n",
    "val numberOfRows = rawData.length\n",
    "val numberOfColumns = rawData.head.length\n",
    "println(s\"Number of rows in dataset: $numberOfRows\")\n",
    "println(s\"Number of columns in dataset: $numberOfColumns\")\n",
    "\n",
    "val (data, lookUpTable) = {\n",
    "    val dataWithNumericColumns = (0 until (numberOfColumns - 1)).toVector.foldLeft(rawData) { (d, i) => textColumnToNumeric(d, i)}\n",
    "    categoricalColumnToNumeric(dataWithNumericColumns, numberOfColumns - 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separation by Class\n",
    "\n",
    "In a moment we will need to calculate the probability  of data by the class they belong to. In order to do that, we'll need to first searate our training set by classes. Fairly easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mseparateByClass\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def separateByClass(dataset: Dataset): Map[Data, Vector[Vector[Data]]] = {\n",
    "  dataset.groupBy(_.last)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on a mock dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmockDataset\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mNumeric\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m3.393533211\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m2.331273381\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m3.110073483\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.781539638\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m1.343808831\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m3.368360954\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m3.582294042\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m4.67917911\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m2.280362439\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m2.866990263\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m7.42346942\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m4.696522875\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m5.745051997\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m3.533989803\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m9.172168622\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m2.511101045\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m7.7922783481\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m3.424088941\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m7.939820817\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.791637231\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m))\n",
       ")\n",
       "\u001b[36mseparated\u001b[39m: \u001b[32mMap\u001b[39m[\u001b[32mData\u001b[39m, \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]]] = \u001b[33mMap\u001b[39m(\n",
       "  Numeric(1.0) -> \u001b[33mVector\u001b[39m(\n",
       "    \u001b[33mVector\u001b[39m(Numeric(7.42346942), Numeric(4.696522875), Numeric(1.0)),\n",
       "    \u001b[33mVector\u001b[39m(Numeric(5.745051997), Numeric(3.533989803), Numeric(1.0)),\n",
       "    \u001b[33mVector\u001b[39m(Numeric(9.172168622), Numeric(2.511101045), Numeric(1.0)),\n",
       "    \u001b[33mVector\u001b[39m(Numeric(7.7922783481), Numeric(3.424088941), Numeric(1.0)),\n",
       "    \u001b[33mVector\u001b[39m(Numeric(7.939820817), Numeric(0.791637231), Numeric(1.0))\n",
       "  ),\n",
       "  Numeric(0.0) -> \u001b[33mVector\u001b[39m(\n",
       "    \u001b[33mVector\u001b[39m(Numeric(3.393533211), Numeric(2.331273381), Numeric(0.0)),\n",
       "    \u001b[33mVector\u001b[39m(Numeric(3.110073483), Numeric(1.781539638), Numeric(0.0)),\n",
       "    \u001b[33mVector\u001b[39m(Numeric(1.343808831), Numeric(3.368360954), Numeric(0.0)),\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mockDataset = Vector(\n",
    "  (3.393533211, 2.331273381, 0),\n",
    "  (3.110073483, 1.781539638, 0),\n",
    "  (1.343808831, 3.368360954, 0),\n",
    "  (3.582294042, 4.67917911, 0),\n",
    "  (2.280362439, 2.866990263, 0),\n",
    "  (7.42346942, 4.696522875, 1),\n",
    "  (5.745051997, 3.533989803, 1),\n",
    "  (9.172168622, 2.511101045, 1),\n",
    "  (7.7922783481, 3.424088941, 1),\n",
    "  (7.939820817, 0.791637231, 1)\n",
    ") map { case (x1, x2, y) => Vector(Numeric(x1), Numeric(x2), Numeric(y))}\n",
    "\n",
    "val separated = separateByClass(mockDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. We can see it is working as expected by inspecting the rows that form each group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Dataset\n",
    "\n",
    "The next step is to obtain two very important statistics for each feature (column) in the dataset:\n",
    "\n",
    " - Mean.\n",
    " - Standard Deviation.\n",
    " \n",
    "In order to be a bit more efficient, we'll collect each of these statistics along with the row count per feature in one pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msummarizeDataset\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarizeDataset(dataset: Dataset) = {\n",
    "  val numberOfColumns = dataset.head.length\n",
    "\n",
    "  val means = getColumnsMeans(dataset)\n",
    "  val standardDeviations = getColumnsStandardDeviations(dataset, means)\n",
    "  val counts = (1 to dataset.head.length).toVector.map(_ => dataset.length)\n",
    "\n",
    "  assert(List(means.length, standardDeviations.length, counts.length).forall(_ == numberOfColumns))\n",
    "\n",
    "  // We ignore the labels column\n",
    "  (0 until numberOfColumns - 1).toVector.map(i => (means(i).get, standardDeviations(i).get, counts(i)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizeDataset(mockDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first triplet corresponds to the statistics of X1 and the second to the statistics of X2. In a table:\n",
    "\n",
    "|                    | X1                | X2                |\n",
    "|--------------------|-------------------|-------------------|\n",
    "| __Mean__               | 5.178286121009999 | 2.9984683241      |\n",
    "| __Standard Deviation__ | 2.766534398702562 | 1.218556343617447 |\n",
    "| __Count__              | 10                | 10                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Data by Class\n",
    "\n",
    "With these two functions implemented we have all we need to summarize each subset of data corresponding to each class. We'll keep each summary in a map where the label is the class and the value is a list of triplets (mean, standard deviation and count) per feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizeByClass(dataset: Dataset) = {\n",
    "  val separated = separateByClass(dataset)\n",
    "\n",
    "  separated.mapValues(summarizeDataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val summaries = summarizeByClass(mockDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Probability Density Function\n",
    "\n",
    "Calculating the probability of observing a given value is quite hard. One of the tricks used to do it is to _assume_ that this value is drawn from a distribution (in this case, a Gaussian distribution).\n",
    "\n",
    "The good thing about Gaussian distributions is that they can be summarized with only two numbers: Mean and standard deviation. With the mighty power of math we can _estimate_ the probability of some value _X_:\n",
    "\n",
    "$$ P(X)=\\frac{1}{\\sqrt{2*\\pi}*\\sigma}e^{-(\\frac{(X - \\mu)^2}{(2*\\sigma)^2})}$$\n",
    "\n",
    "Let's create a function that does the heavy lifting for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateProbability(x: Double, mean: Double, standardDeviation: Double) = {\n",
    "  val exponent = math.exp(-(math.pow(x - mean, 2) / (2 * standardDeviation * standardDeviation)))\n",
    "  (1.0 / (math.sqrt(2 * math.Pi) * standardDeviation)) * exponent\n",
    "}\n",
    "\n",
    "calculateProbability(1.0, 1.0, 1.0)\n",
    "calculateProbability(2.0, 1.0, 1.0)\n",
    "calculateProbability(0.0, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Probabilities\n",
    "\n",
    "Now we need to compute the class probabilities for new data given the statistics summary per feature. These are calculated separately for each class in the dataset.\n",
    "\n",
    "The probability that some value corresponds to a class is:\n",
    "\n",
    "$$ P(class|data)=P(X|data)*P(class)$$\n",
    "\n",
    "This is not exactly the Bayes Theorem. What does dropping the denominator causes? Well, $P(data)$ is actually a normalization factor that makes sure that the value we are calculating falls in the range [0, 1] and, hence, can be considered an actual probability. Given that we are more interested in the classification task rather than the probability itself, we can save ourselves computation resources and still be sure we'll classify each data point right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateClassProbabilities(summaries: Map[Data, Vector[(Double, Double, Int)]], row: Vector[Data]) = {\n",
    "  val totalRows = summaries.foldLeft(0){ (accum, entry) =>\n",
    "    entry match {\n",
    "      case (_, summary) => accum + summary.head._3\n",
    "    }\n",
    "  }\n",
    "\n",
    "  summaries.mapValues { summaries =>\n",
    "    var a = summaries.head._3 / totalRows.toDouble\n",
    "\n",
    "    // Class probability\n",
    "    summaries.indices.foldLeft(summaries.head._3 / totalRows.toDouble) { (classProbability, i) =>\n",
    "      val (mean, standardDeviation, _) = summaries(i)\n",
    "      classProbability * calculateProbability(getNumericValue(row(i)).get, mean, standardDeviation)\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val probabilities = calculateClassProbabilities(summaries, mockDataset.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the probability of the first row belonging to the first class (0) is quite higher that the probability it belongs to the second class (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes\n",
    "\n",
    "We can now make predictions using all the functions we've implemented so far. Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictNB(summaries: Map[Data, Vector[(Double, Double, Int)]], row: Vector[Data]): Data = {\n",
    "  val probabilities = calculateClassProbabilities(summaries, row)\n",
    "\n",
    "  val (Some(bestLabel), _) = probabilities.foldLeft((None: Option[Data], -1.0)) { (bestLabelAndProb, entry) =>\n",
    "      entry match {\n",
    "        case (label, classProbability) =>\n",
    "          val (bestLabel, bestProbability) = bestLabelAndProb\n",
    "\n",
    "          if (bestLabel.isEmpty || classProbability > bestProbability) {\n",
    "            (Some(label), classProbability)\n",
    "          } else {\n",
    "            bestLabelAndProb\n",
    "          }\n",
    "      }\n",
    "  }\n",
    "\n",
    "  bestLabel\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveBayes(train: Dataset, test: Dataset) = {\n",
    "  val summaries = summarizeByClass(train)\n",
    "\n",
    "  test.map { row =>\n",
    "    predictNB(summaries, row)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good.\n",
    "\n",
    "Let's now use our new algorithm to test it on the Iris dataset.\n",
    "\n",
    "We'll start by running a baseline model on it and then our freshly implemented Gaussian Naïve Bayes algorithm and then we will compare their performance.\n",
    "\n",
    "As a baseline we will use a __zero rule classifier__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Normalize data\n",
    "val minMax = getDatasetMinAndMax(data)\n",
    "val normalizedData = normalizeDataset(data, minMax)\n",
    "\n",
    "val baselineAccuracy = evaluateAlgorithmUsingTrainTestSplit[Numeric](\n",
    "        normalizedData, \n",
    "        (train, test, parameters) => zeroRuleClassifier(train, test), \n",
    "        Map.empty, \n",
    "        accuracy, \n",
    "        trainProportion=0.8)\n",
    "\n",
    "println(s\"Zero Rule Algorithm accuracy: $baselineAccuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val naiveBayesAccuracy = evaluateAlgorithmUsingTrainTestSplit[Numeric](\n",
    "    normalizedData,\n",
    "    (train, test, parameters) => naiveBayes(train, test),\n",
    "    Map.empty,\n",
    "    accuracy,\n",
    "    trainProportion=0.8)\n",
    "\n",
    "println(s\"Naive Bayes accuracy: $naiveBayesAccuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! It's evident that the 93.334% accuracy achieved by our Gaussian Naïve Bayes is dramatically better that the ine achieved by the baseline model (23.334%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
