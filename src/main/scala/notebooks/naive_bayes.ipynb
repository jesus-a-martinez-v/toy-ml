{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naïve Bayes\n",
    "\n",
    "What a fun name for a classifier, don't you think? :)\n",
    "\n",
    "Naïve Bayes, simply put, refer to a family of algorithms that apply the Bayes' theorem with the _naïve_ assumption that all features are independent from each other (Hence the name). \n",
    "\n",
    "Although there are (very) rare situations where all features are independent, most of the time there are at least some degree of correlation between two or more of them. For instance, one could infer that an object that is a fruit, is red, has a somewhat round shape and grows in trees is most likely an apple. Why? Because each feature gives us more information about the others and that increases our confidence in the output. Nonetheless, Naïve Bayes algorithm make the strong assumption that in any instance this correlation occurs.\n",
    "\n",
    "It doesn't sound like an accurate algorithm, right? Well, the thing is that in real life, despite this naïvety, it works really well and it also facilitates the computation __a lot__.\n",
    "\n",
    "Bayes' Theorem gives us a way to calculate the probability of a piece of data belonging to a particular class, given our prior knowledge.\n",
    "\n",
    "$$ P(class|data)=\\frac{P(data|class)*P(class)}{P(data)} $$\n",
    "\n",
    "It is somewhat confusing, but what this formula says is \"The posterior probability of the class given the data is defined as the prior probability of that data given the class by the likelihood of the class divided by the evidence of the data\". So much clearer now! (Fingers crossed)\n",
    "\n",
    "A nicer way to represent this formula is:\n",
    "\n",
    "$$ posterior=\\frac{prior*likelihood}{evidence} $$\n",
    "\n",
    "Let's start our implementation by loading the code and libraries we'll need. We will build our solution on top of the ones we implemented in the [previous notebook](https://github.com/jesus-a-martinez-v/toy-ml/blob/master/src/main/scala/notebooks/lperceptron.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Perceptron.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                                     , Perceptron._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.github.tototoshi::scala-csv:1.3.5`\n",
    "import $file.^.datasmarts.ml.toy.scripts.Perceptron, Perceptron._\n",
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We'll use the [Iris Flower](http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data) dataset. It involves the prediction of flower species given measurements of iris flowers. \n",
    "\n",
    "Let's load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in dataset: 150\n",
      "Number of column in dataset: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mBASE_DATA_PATH\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"../../resources/data\"\u001b[39m\n",
       "\u001b[36msonarPath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"../../resources/data/12/iris.csv\"\u001b[39m\n",
       "\u001b[36mrawData\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(Text(5.1), Text(3.5), Text(1.4), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.9), Text(3.0), Text(1.4), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.7), Text(3.2), Text(1.3), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.6), Text(3.1), Text(1.5), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.0), Text(3.6), Text(1.4), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.4), Text(3.9), Text(1.7), Text(0.4), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.6), Text(3.4), Text(1.4), Text(0.3), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.0), Text(3.4), Text(1.5), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.4), Text(2.9), Text(1.4), Text(0.2), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(4.9), Text(3.1), Text(1.5), Text(0.1), Text(Iris-setosa)),\n",
       "  \u001b[33mVector\u001b[39m(Text(5.4), Text(3.7), Text(1.5), Text(0.2), Text(Iris-setosa)),\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mnumberOfRows\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m150\u001b[39m\n",
       "\u001b[36mnumberOfColumns\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m5\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(Numeric(5.1), Numeric(3.5), Numeric(1.4), Numeric(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(4.9), Numeric(3.0), Numeric(1.4), Numeric(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(4.7), Numeric(3.2), Numeric(1.3), Numeric(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(4.6), Numeric(3.1), Numeric(1.5), Numeric(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(5.0), Numeric(3.6), Numeric(1.4), Numeric(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(5.4), Numeric(3.9), Numeric(1.7), Numeric(0.4), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(4.6), Numeric(3.4), Numeric(1.4), Numeric(0.3), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(5.0), Numeric(3.4), Numeric(1.5), Numeric(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(4.4), Numeric(2.9), Numeric(1.4), Numeric(0.2), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(4.9), Numeric(3.1), Numeric(1.5), Numeric(0.1), Numeric(0.0)),\n",
       "  \u001b[33mVector\u001b[39m(Numeric(5.4), Numeric(3.7), Numeric(1.5), Numeric(0.2), Numeric(0.0)),\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mlookUpTable\u001b[39m: \u001b[32mMap\u001b[39m[\u001b[32mData\u001b[39m, \u001b[32mInt\u001b[39m] = \u001b[33mMap\u001b[39m(\n",
       "  Text(Iris-setosa) -> \u001b[32m0\u001b[39m,\n",
       "  Text(Iris-versicolor) -> \u001b[32m1\u001b[39m,\n",
       "  Text(Iris-virginica) -> \u001b[32m2\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val BASE_DATA_PATH = \"../../resources/data\"\n",
    "val sonarPath = s\"$BASE_DATA_PATH/12/iris.csv\"\n",
    "\n",
    "val rawData = loadCsv(sonarPath)\n",
    "val numberOfRows = rawData.length\n",
    "val numberOfColumns = rawData.head.length\n",
    "println(s\"Number of rows in dataset: $numberOfRows\")\n",
    "println(s\"Number of column in dataset: $numberOfColumns\")\n",
    "\n",
    "val (data, lookUpTable) = {\n",
    "    val dataWithNumericColumns = (0 until (numberOfColumns - 1)).toVector.foldLeft(rawData) { (d, i) => textColumnToNumeric(d, i)}\n",
    "    categoricalColumnToNumeric(dataWithNumericColumns, numberOfColumns - 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separation by Class\n",
    "\n",
    "In a moment we will need to calculate the probability  of data by the class they belong to. In order to do that, we'll need to first searate our training set by classes. Fairly easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mseparateByClass\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def separateByClass(dataset: Dataset): Map[Data, Vector[Vector[Data]]] = {\n",
    "  dataset.groupBy(_.last)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on a mock dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmockDataset\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mNumeric\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m3.393533211\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m2.331273381\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m3.110073483\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.781539638\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m1.343808831\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m3.368360954\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m3.582294042\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m4.67917911\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m2.280362439\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m2.866990263\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m7.42346942\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m4.696522875\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m5.745051997\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m3.533989803\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m9.172168622\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m2.511101045\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m7.7922783481\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m3.424088941\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m7.939820817\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m0.791637231\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m))\n",
       ")\n",
       "\u001b[36mseparated\u001b[39m: \u001b[32mMap\u001b[39m[\u001b[32mData\u001b[39m, \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]]] = \u001b[33mMap\u001b[39m(\n",
       "  Numeric(1.0) -> \u001b[33mVector\u001b[39m(\n",
       "    \u001b[33mVector\u001b[39m(Numeric(7.42346942), Numeric(4.696522875), Numeric(1.0)),\n",
       "    \u001b[33mVector\u001b[39m(Numeric(5.745051997), Numeric(3.533989803), Numeric(1.0)),\n",
       "    \u001b[33mVector\u001b[39m(Numeric(9.172168622), Numeric(2.511101045), Numeric(1.0)),\n",
       "    \u001b[33mVector\u001b[39m(Numeric(7.7922783481), Numeric(3.424088941), Numeric(1.0)),\n",
       "    \u001b[33mVector\u001b[39m(Numeric(7.939820817), Numeric(0.791637231), Numeric(1.0))\n",
       "  ),\n",
       "  Numeric(0.0) -> \u001b[33mVector\u001b[39m(\n",
       "    \u001b[33mVector\u001b[39m(Numeric(3.393533211), Numeric(2.331273381), Numeric(0.0)),\n",
       "    \u001b[33mVector\u001b[39m(Numeric(3.110073483), Numeric(1.781539638), Numeric(0.0)),\n",
       "    \u001b[33mVector\u001b[39m(Numeric(1.343808831), Numeric(3.368360954), Numeric(0.0)),\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mockDataset = Vector(\n",
    "  (3.393533211, 2.331273381, 0),\n",
    "  (3.110073483, 1.781539638, 0),\n",
    "  (1.343808831, 3.368360954, 0),\n",
    "  (3.582294042, 4.67917911, 0),\n",
    "  (2.280362439, 2.866990263, 0),\n",
    "  (7.42346942, 4.696522875, 1),\n",
    "  (5.745051997, 3.533989803, 1),\n",
    "  (9.172168622, 2.511101045, 1),\n",
    "  (7.7922783481, 3.424088941, 1),\n",
    "  (7.939820817, 0.791637231, 1)\n",
    ") map { case (x1, x2, y) => Vector(Numeric(x1), Numeric(x2), Numeric(y))}\n",
    "\n",
    "val separated = separateByClass(mockDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. We can see it is working as expected by inspecting the rows that form each group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Dataset\n",
    "\n",
    "The next step is to obtain two very important statistics for each feature (column) in the dataset:\n",
    "\n",
    " - Mean.\n",
    " - Standard Deviation.\n",
    " \n",
    "In order to be a bit more efficient, we'll collect each of these statistics along with the row count per feature in one pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msummarizeDataset\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarizeDataset(dataset: Dataset) = {\n",
    "  val numberOfColumns = dataset.head.length\n",
    "\n",
    "  val means = getColumnsMeans(dataset)\n",
    "  val standardDeviations = getColumnsStandardDeviations(dataset, means)\n",
    "  val counts = (1 to dataset.head.length).toVector.map(_ => dataset.length)\n",
    "\n",
    "  assert(List(means.length, standardDeviations.length, counts.length).forall(_ == numberOfColumns))\n",
    "\n",
    "  // We ignore the labels column\n",
    "  (0 until numberOfColumns - 1).toVector.map(i => (means(i).get, standardDeviations(i).get, counts(i)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres5\u001b[39m: \u001b[32mVector\u001b[39m[(\u001b[32mDouble\u001b[39m, \u001b[32mDouble\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mVector\u001b[39m(\n",
       "  (\u001b[32m5.178286121009999\u001b[39m, \u001b[32m2.766534398702562\u001b[39m, \u001b[32m10\u001b[39m),\n",
       "  (\u001b[32m2.9984683241\u001b[39m, \u001b[32m1.218556343617447\u001b[39m, \u001b[32m10\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizeDataset(mockDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first triplet corresponds to the statistics of X1 and the second to the statistics of X2. In a table:\n",
    "\n",
    "|                    | X1                | X2                |\n",
    "|--------------------|-------------------|-------------------|\n",
    "| __Mean__               | 5.178286121009999 | 2.9984683241      |\n",
    "| __Standard Deviation__ | 2.766534398702562 | 1.218556343617447 |\n",
    "| __Count__              | 10                | 10                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Data by Class\n",
    "\n",
    "With these two functions implemented we have all we need to summarize each subset of data corresponding to each class. We'll keep each summary in a map where the label is the class and the value is a list of triplets (mean, standard deviation and count) per feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msummarizeByClass\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarizeByClass(dataset: Dataset) = {\n",
    "  val separated = separateByClass(dataset)\n",
    "\n",
    "  separated.mapValues(summarizeDataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres7\u001b[39m: \u001b[32mMap\u001b[39m[\u001b[32mData\u001b[39m, \u001b[32mVector\u001b[39m[(\u001b[32mDouble\u001b[39m, \u001b[32mDouble\u001b[39m, \u001b[32mInt\u001b[39m)]] = \u001b[33mMap\u001b[39m(\n",
       "  Numeric(1.0) -> \u001b[33mVector\u001b[39m(\n",
       "    (\u001b[32m7.61455784082\u001b[39m, \u001b[32m1.2344126956449888\u001b[39m, \u001b[32m5\u001b[39m),\n",
       "    (\u001b[32m2.9914679790000003\u001b[39m, \u001b[32m1.4541931384601618\u001b[39m, \u001b[32m5\u001b[39m)\n",
       "  ),\n",
       "  Numeric(0.0) -> \u001b[33mVector\u001b[39m(\n",
       "    (\u001b[32m2.7420144012\u001b[39m, \u001b[32m0.9265683289298018\u001b[39m, \u001b[32m5\u001b[39m),\n",
       "    (\u001b[32m3.0054686692\u001b[39m, \u001b[32m1.1073295894898725\u001b[39m, \u001b[32m5\u001b[39m)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizeByClass(mockDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Probability Density Function\n",
    "\n",
    "Calculating the probability of observing a given value is quite hard. One of the tricks used to do it is to _assume_ that this value is drawn from a distribution (in this case, a Gaussian distribution).\n",
    "\n",
    "The good thing about Gaussian distributions is that they can be summarized with only two numbers: Mean and standard deviation. With the mighty power of math we can _estimate_ the probability of some value _X_:\n",
    "\n",
    "$$ P(X)=\\frac{1}{\\sqrt{2*\\pi}*\\sigma}e^{-(\\frac{(X - \\mu)^2}{(2*\\sigma)^2})}$$\n",
    "\n",
    "Let's create a function that does the heavy lifting for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcalculateProbability\u001b[39m\n",
       "\u001b[36mres8_1\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.3989422804014327\u001b[39m\n",
       "\u001b[36mres8_2\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.24197072451914337\u001b[39m\n",
       "\u001b[36mres8_3\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.24197072451914337\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculateProbability(x: Double, mean: Double, standardDeviation: Double) = {\n",
    "  val exponent = math.exp(-(math.pow(x - mean, 2) / (2 * standardDeviation * standardDeviation)))\n",
    "  (1.0 / (math.sqrt(2 * math.Pi) * standardDeviation)) * exponent\n",
    "}\n",
    "\n",
    "calculateProbability(1.0, 1.0, 1.0)\n",
    "calculateProbability(2.0, 1.0, 1.0)\n",
    "calculateProbability(0.0, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
