{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression\n",
    "\n",
    "As we have seen before, a Simple Linear Regression is just a more concrete instance of a general regression algorithm that we will study today, called Multivariate Linear Regression.\n",
    "\n",
    "As it name suggests, it still asumes a linear relationship between the inputs and the output, only that this time we can have many variables in the input. Of course, with more dimensions, the visualization of the relation between independent and dependent variables cannot be seen as a straight line anymore, but what we must remember is that the algorithm finds the formula that describes some hyperplane or surface that \"linearly\" separates the inputs and the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "At the core of many machine learning algorithms lies, really, an optimization problem: We want to find the set of parameters and hyperparameters that minimizes the error that the algorithm makes on some training data, compared with the actual labels.\n",
    "\n",
    "One of the most frequently used algorithms for this task is __Gradient Descent__. Here's a [really good video](https://www.youtube.com/watch?v=umAeJ7LMCfU) that explains it.\n",
    "\n",
    "We know from calculus that the gradient or derivative of a functions can be interpreted as the slope of the line tangent to that function at a certain point. Moreover, it is the direction of steepest ascent at that point. Hence, if we want to know the direction of the steepest descent, we just put a minus sign before the gradient :)\n",
    "\n",
    "Some key aspects of gradient descent are:\n",
    "\n",
    " - Each example is shown to the algorithm, who makes a prediction on it. Then the error is calculated and the weights or parameters of the model are tweaked a bit to reduce that prediction error.\n",
    " - A hyperparameter called _learning rate_ controls the speed of the learning process. A learning rate too big causes the algorithm to overshoot frequently and hurts the performance, even to the point that the model never converges. A learning rate too small will slow down the progress a lot. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the libraries we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                                                 , SimpleLinearRegression._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.github.tototoshi::scala-csv:1.3.5`\n",
    "import $file.^.datasmarts.ml.toy.scripts.SimpleLinearRegression, SimpleLinearRegression._\n",
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "In this occasion we will use the [Wine Quality Dataset](https://raw.githubusercontent.com/jesus-a-martinez-v/toy-ml/master/src/main/resources/data/8/winequality-white.csv). \n",
    "\n",
    "This dataset involves predicting the quality of white wines on a scale given measures of each of them. Although it is originally a multiclass classification problem, it can also be framed as a regression one. \n",
    "\n",
    "Let's load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in dataset: 4898\n",
      "Number of columns in dataset: 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mBASE_DATA_PATH\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"../../resources/data\"\u001b[39m\n",
       "\u001b[36mwineQualityDataPath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"../../resources/data/8/winequality-white.csv\"\u001b[39m\n",
       "\u001b[36mrawData\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\n",
       "    Text(7),\n",
       "    Text(0.27),\n",
       "    Text(0.36),\n",
       "    Text(20.7),\n",
       "    Text(0.045),\n",
       "    Text(45),\n",
       "    Text(170),\n",
       "    Text(1.001),\n",
       "    Text(3),\n",
       "    Text(0.45),\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mnumberOfRows\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m4898\u001b[39m\n",
       "\u001b[36mnumberOfColumns\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m12\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mData\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\n",
       "    Numeric(7.0),\n",
       "    Numeric(0.27),\n",
       "    Numeric(0.36),\n",
       "    Numeric(20.7),\n",
       "    Numeric(0.045),\n",
       "    Numeric(45.0),\n",
       "    Numeric(170.0),\n",
       "    Numeric(1.001),\n",
       "    Numeric(3.0),\n",
       "    Numeric(0.45),\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val BASE_DATA_PATH = \"../../resources/data\"\n",
    "val wineQualityDataPath = s\"$BASE_DATA_PATH/8/winequality-white.csv\"\n",
    "\n",
    "val rawData = loadCsv(wineQualityDataPath)\n",
    "val numberOfRows = rawData.length\n",
    "val numberOfColumns = rawData.head.length\n",
    "println(s\"Number of rows in dataset: $numberOfRows\")\n",
    "println(s\"Number of columns in dataset: $numberOfColumns\")\n",
    "\n",
    "val data = (0 until numberOfColumns).toVector.foldLeft(rawData) { (d, i) => textColumnToNumeric(d, i)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "Our first step is to implement a function that given a row and a set of coefficients, returns a prediction to us.\n",
    "\n",
    "Given that we will be dealing with lots of vectors from now on, let's implement a helper that allow us to update a certain position inside a vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mupdatedVector\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def updatedVector[T](vector: Vector[T], newValue: T, index: Int): Vector[T] = {\n",
    "  val (firstHalf, secondHalf) = vector.splitAt(index)\n",
    "  firstHalf ++ Vector(newValue) ++ secondHalf.tail\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Let's implement the actual prediction function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mpredictLinearRegression\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predictLinearRegression(row: Vector[Data], coefficients: Vector[Double]): Double = {\n",
    "  val indices = row.indices.init\n",
    "\n",
    "  // The first coefficient is the bias term, so we don't multiply it by any variable in row (that's why it is added at the end)  \n",
    "  indices.foldLeft(0.0) { (accumulator, index) =>\n",
    "    accumulator + coefficients(index + 1) * getNumericValue(row(index)).get\n",
    "  } + coefficients.head\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our implementation with some dummy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Numeric(1.0), Predicted 1.2000000000000002\n",
      "Expected Numeric(3.0), Predicted 2.0\n",
      "Expected Numeric(3.0), Predicted 3.6\n",
      "Expected Numeric(2.0), Predicted 2.8000000000000003\n",
      "Expected Numeric(5.0), Predicted 4.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mmockData\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mNumeric\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m1.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m2.0\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m3.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m4.0\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m3.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m3.0\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m2.0\u001b[39m)),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[33mNumeric\u001b[39m(\u001b[32m5.0\u001b[39m), \u001b[33mNumeric\u001b[39m(\u001b[32m5.0\u001b[39m))\n",
       ")\n",
       "\u001b[36mmockCoefficients\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mVector\u001b[39m(\u001b[32m0.4\u001b[39m, \u001b[32m0.8\u001b[39m)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mockData = Vector((1, 1), (2, 3), (4, 3), (3, 2), (5, 5)).map { case (x, y) => Vector(Numeric(x), Numeric(y)) }\n",
    "val mockCoefficients = Vector(0.4, 0.8)\n",
    "\n",
    "mockData.foreach(row => println(s\"Expected ${row.last}, Predicted ${predictLinearRegression(row, mockCoefficients)}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to implement stochastic gradient descent to optimize our coefficient values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Coefficients\n",
    "\n",
    "Stochastic Gradient Descent (SGD) requires two parameters:\n",
    "\n",
    " - __Learning Rate__: It is used to control the amount of correction each parameter will receive at a time.\n",
    " - __Number of epochs__: Number of times the algorithm will loop over all the data, updating the coefficients.\n",
    " \n",
    "The outline of the algorithm is as follows:\n",
    "\n",
    " 1. Loop over each epoch.\n",
    " 2. Loop over each row in the training set.\n",
    " 3. Loop over each coefficient and update it for a row in an epoch.\n",
    " \n",
    "The first coefficient is a bias term and it is not associated to any input. Its update formula is:\n",
    "\n",
    "$$ b_0 = b_0-\\alpha*error $$\n",
    "\n",
    "All the remaining coefficients are updated with this formula:\n",
    "\n",
    "$$ b_i = b_i-\\alpha*error*x_i  $$\n",
    "\n",
    "Where $\\alpha$ is the learning rate and $error$ is defined as:\n",
    "\n",
    "$$ error = prediction - actual $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcoefficientsLinearRegressionSgd\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def coefficientsLinearRegressionSgd(train: Dataset, learningRate: Double, numberOfEpochs: Int) = {\n",
    "  var coefficients = Vector.fill(train.head.length)(0.0)\n",
    "\n",
    "  for {\n",
    "    epoch <- 1 to numberOfEpochs\n",
    "    row <- train\n",
    "  } {\n",
    "    val predicted = predictLinearRegression(row, coefficients)\n",
    "    val actual = getNumericValue(row.last).get\n",
    "    val error = predicted - actual\n",
    "    \n",
    "    val bias = coefficients.head - learningRate * error\n",
    "    val indices = row.indices.init\n",
    "\n",
    "    val remainingCoefficients = indices.foldLeft(coefficients) { (c, index) =>\n",
    "      updatedVector(c, c(index + 1) - learningRate * error * getNumericValue(row(index)).get, index + 1)\n",
    "    }\n",
    "    \n",
    "    coefficients = Vector(bias) ++ remainingCoefficients.tail\n",
    "  }\n",
    "\n",
    "  coefficients\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the coefficients for our mock dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres6\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mVector\u001b[39m(\u001b[32m0.22998234937311363\u001b[39m, \u001b[32m0.8017220304137576\u001b[39m)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficientsLinearRegressionSgd(mockData, 0.001, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Now that we have all the pieces, defining a linear regression is easy. Let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mlinearRegressionSgd\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linearRegressionSgd(train: Dataset, test: Dataset, parameters: Parameters) = {\n",
    "  val learningRate = parameters(\"learningRate\").asInstanceOf[Double]\n",
    "  val numberOfEpochs = parameters(\"numberOfEpochs\").asInstanceOf[Int]\n",
    "\n",
    "  val coefficients = coefficientsLinearRegressionSgd(train, learningRate, numberOfEpochs)\n",
    "    \n",
    "  test.map { row =>\n",
    "    Numeric(predictLinearRegression(row, coefficients))\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. We just need to unpack the relevant parameters, use SGD to obtain the coefficients and then use them to make predictions on the test set.\n",
    "\n",
    "Let's now use our new algorithm to test it on the Wine Quality Dataset.\n",
    "\n",
    "We'll start by running a baseline model on it and then our freshly implemented linear regression algorithm and then we will compare their performance.\n",
    "\n",
    "As a baseline we will use a __zero rule regressor__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Rule Regressor RMSE: 0.15054440983011522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mminMax\u001b[39m: \u001b[32mMinMaxData\u001b[39m = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m3.8\u001b[39m, \u001b[32m14.2\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.08\u001b[39m, \u001b[32m1.1\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.0\u001b[39m, \u001b[32m1.66\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.6\u001b[39m, \u001b[32m65.8\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.009\u001b[39m, \u001b[32m0.346\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m2.0\u001b[39m, \u001b[32m289.0\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m9.0\u001b[39m, \u001b[32m440.0\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.98711\u001b[39m, \u001b[32m1.03898\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m2.72\u001b[39m, \u001b[32m3.82\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m0.22\u001b[39m, \u001b[32m1.08\u001b[39m)),\n",
       "  \u001b[33mSome\u001b[39m((\u001b[32m8.0\u001b[39m, \u001b[32m14.2\u001b[39m)),\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mnormalizedData\u001b[39m: \u001b[32mDataset\u001b[39m = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\n",
       "    Numeric(0.30769230769230776),\n",
       "    Numeric(0.18627450980392157),\n",
       "    Numeric(0.21686746987951808),\n",
       "    Numeric(0.308282208588957),\n",
       "    Numeric(0.10682492581602374),\n",
       "    Numeric(0.14982578397212543),\n",
       "    Numeric(0.37354988399071926),\n",
       "    Numeric(0.26778484673221237),\n",
       "    Numeric(0.25454545454545446),\n",
       "    Numeric(0.26744186046511625),\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mbaselineRmse\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.15054440983011522\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Normalize data\n",
    "val minMax = getDatasetMinAndMax(data)\n",
    "val normalizedData = normalizeDataset(data, minMax)\n",
    "\n",
    "val baselineRmse = evaluateAlgorithmUsingTrainTestSplit[Numeric](\n",
    "        normalizedData, \n",
    "        (train, test, parameters) => zeroRuleRegressor(train, test), \n",
    "        Map.empty, \n",
    "        rootMeanSquaredError, \n",
    "        trainProportion=0.8)\n",
    "\n",
    "println(s\"Zero Rule Regressor RMSE: $baselineRmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regressor RMSE: 0.12836132933306205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mlinearRegressionRmse\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.12836132933306205\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val linearRegressionRmse = evaluateAlgorithmUsingTrainTestSplit[Numeric](\n",
    "        normalizedData, \n",
    "        linearRegressionSgd, \n",
    "        Map(\"numberOfEpochs\" -> 50, \"learningRate\" -> 0.001), \n",
    "        rootMeanSquaredError, \n",
    "        trainProportion=0.8)\n",
    "\n",
    "println(s\"Linear Regressor RMSE: $linearRegressionRmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our linear regression algorithm performs a little bit better than the baseline we defined above. We could squeeze more predictive power by tweaking the learning rate and the number of epochs. Feel free to experiment! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
